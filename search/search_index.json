{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"GitHub Self-Hosted runners @ AWS","text":"<p>This Terraform module creates the required infrastructure needed to host GitHub Actions self-hosted, auto-scaling runners on AWS spot instances. It provides the required logic to handle the life cycle for scaling up and down using a set of AWS Lambda functions. Runners are scaled down to zero to avoid costs when no workflows are active.</p>"},{"location":"index.html#motivation","title":"Motivation","text":"<p>GitHub Actions <code>self-hosted</code> runners provide a flexible option to run CI workloads on the infrastructure of your choice. However, currently GitHub does not provide tooling to automate the creation and scaling of action runners. This module creates the AWS infrastructure to host action runners on spot instances. It also provides lambda modules to orchestrate the life cycle of the action runners.</p> <p>Lambda was selected as the preferred runtime for two primary reasons. Firstly, it enables the development of compact components with limited access to AWS and GitHub. Secondly, it offers a scalable configuration with minimal expenses, applicable at both the repository and organizational levels. The Lambda functions will be responsible for provisioning Linux-based EC2 instances equipped with Docker to handle CI workloads compatible with Linux and/or Docker. The primary objective is to facilitate Docker-based workloads.</p> <p>A pertinent question may arise: why not opt for Kubernetes? The current strategy aligns closely with the implementation of GitHub's action runners. The chosen approach involves installing the runner on a host where the necessary software is readily available, maintaining proximity to GitHub's existing practices. Another viable option could be AWS Auto Scaling groups. However, this alternative usually demands broader permissions at the instance level from GitHub. Additionally, managing the scaling process, both up and down, becomes a non-trivial task in this scenario.</p>"},{"location":"index.html#overview","title":"Overview","text":"<p>The moment a GitHub action workflow requiring a <code>self-hosted</code> runner is triggered, GitHub will try to find a runner which can execute the workload. See additional notes for how the selection is made. This module reacts to GitHub's <code>workflow_job</code> event for the triggered workflow and creates a new runner if necessary.</p> <p>For receiving the <code>workflow_job</code> event by the webhook (lambda), a webhook needs to be created in GitHub. The <code>check_run</code> option was dropped from version 2.x. The following options to send the event are supported.</p> <ul> <li>Create a GitHub app, define a webhook and subscribe the app to the <code>workflow_job</code> event.</li> <li>Create a webhook on enterprise, org or repo level, define a webhook and subscribe the app to the <code>workflow_job</code> event.</li> </ul> <p>In AWS an API gateway endpoint is created that is able to receive the GitHub webhook events via HTTP post. The gateway triggers the webhook lambda which will verify the signature of the event. This check guarantees the event is sent by the GitHub App. The lambda only handles <code>workflow_job</code> events with status <code>queued</code> and matching the runner labels. The accepted events are posted on a SQS queue. Messages on this queue will be delayed for a configurable amount of seconds (default 30 seconds) to give the available runners time to pick up this build.</p> <p>The \"Scale Up Runner\" Lambda actively monitors the SQS queue, processing incoming events. The Lambda conducts a series of checks to determine the necessity of creating a new EC2 spot instance. For instance, it refrains from creating an instance if a build is already initiated by an existing runner or if the maximum allowable number of runners has been reached.</p> <p>The Lambda first requests a JIT configuration or registration token from GitHub, which is needed later by the runner to register itself. This avoids the case that the EC2 instance, which later in the process will install the agent, needs administration permissions to register the runner. Next, the EC2 spot instance is created via the launch template. The launch template defines the specifications of the required instance and contains a <code>user_data</code> script. This script will install the required software and configure it. The registration token for the action runner is stored in the parameter store (SSM), from which the user data script will fetch it and delete it once it has been retrieved. Once the user data script is finished, the action runner should be online, and the workflow will start in seconds.</p> <p>The current method for scaling down runners employs a straightforward approach: at predefined intervals, the Lambda conducts a thorough examination of each runner (instance) to assess its activity. If a runner is found to be idle, it is deregistered from GitHub, and the associated AWS instance is terminated. Presently, no alternative method appears available for achieving a more gradual scaling down process.</p> <p>To address potential delays in downloading the GitHub Action Runner distribution, a lambda function has been implemented to synchronize the action runner binary from GitHub to an S3 bucket. This ensures that the EC2 instance can retrieve the distribution from the S3 bucket, mitigating the need to rely on internet downloads, which can occasionally take more than 10 minutes.</p> <p>Sensitive information such as secrets and private keys is stored securely in the SSM Parameter Store. These values undergo encryption using either the default KMS key for SSM or a custom KMS key, depending on the specified configuration.</p> <p></p> <p>Permission are managed in several places. Below are the most important ones. For details check the Terraform sources.</p> <ul> <li>The GitHub App requires access to actions and to publish <code>workflow_job</code> events to the AWS webhook (API gateway).</li> <li>The scale up lambda should have access to EC2 for creating and tagging instances.</li> <li>The scale down lambda should have access to EC2 to terminate instances.</li> </ul> <p>Besides these permissions, the lambdas also need permission to CloudWatch (for logging and scheduling), SSM and S3. For more details about the required permissions see the documentation of the IAM module which uses permission boundaries.</p>"},{"location":"additional_notes.html","title":"Runner Labels","text":"<p>Some CI systems require that all labels match between a job and a runner. In the case of GitHub Actions, workflows will be assigned to runners which have all the labels requested by the workflow, however it is not necessary the workflow mentions all labels.</p> <p>Labels specify the capabilities the runners have. The labels in the workflow are the capabilities needed. If the capabilities requested by the workflow are provided by the runners, there is match.  </p> <p>Examples:</p> Runner Labels Workflow runs-on: Result 'self-hosted', 'Linux', 'X64' self-hosted matches 'self-hosted', 'Linux', 'X64' Linux matches 'self-hosted', 'Linux', 'X64' X64 matches 'self-hosted', 'Linux', 'X64' [ self-hosted, Linux ] matches 'self-hosted', 'Linux', 'X64' [ self-hosted, X64 ] matches 'self-hosted', 'Linux', 'X64' [ self-hosted, Linux, X64 ] matches 'self-hosted', 'Linux', 'X64' other1 no match 'self-hosted', 'Linux', 'X64' [ self-hosted, other2 ] no match 'self-hosted', 'Linux', 'X64' [ self-hosted, Linux, X64, other2 ] no match 'self-hosted', 'Linux', 'X64', 'custom3' custom3 matches 'self-hosted', 'Linux', 'X64', 'custom3' [ custom3, Linux ] matches 'self-hosted', 'Linux', 'X64', 'custom3' [ custom3, X64 ] matches 'self-hosted', 'Linux', 'X64', 'custom3' [ custom3, other7 ] no match <p>If default labels are removed:</p> Runner Labels Workflow runs-on: Result 'custom5' custom5 matches 'custom5' self-hosted no match 'custom5' Linux no match 'custom5' [ self-hosted, Linux ] no match 'custom5' [ custom5, self-hosted, Linux ] no match"},{"location":"configuration.html","title":"Configuration","text":""},{"location":"configuration.html#major-configuration-considarations","title":"Major configuration considarations","text":"<p>To be able to support a number of use-cases, the module has quite a lot of configuration options. We tried to choose reasonable defaults. Several examples also show the main cases of how to configure the runners.</p> <ul> <li>Org vs Repo level. You can configure the module to connect the runners in GitHub on an org level and share the runners in your org, or set the runners on repo level and the module will install the runner to the repo. There can be multiple repos but runners are not shared between repos.</li> <li>Multi-Runner module. This modules allows you to create multiple runner configurations with a single webhook and single GitHub App to simplify deployment of different types of runners. Check the detailed module documentation for more information or checkout the multi-runner example.</li> <li>Workflow job event. You can configure the webhook in GitHub to send workflow job events to the webhook. Workflow job events were introduced by GitHub in September 2021 and are designed to support scalable runners. We advise using the workflow job event when possible.</li> <li>Linux vs Windows. You can configure the OS types linux and win. Linux will be used by default.</li> <li>Re-use vs Ephemeral. By default runners are re-used, until detected idle. Once idle they will be removed from the pool. To improve security we are introducing ephemeral runners. Those runners are only used for one job. Ephemeral runners only work in combination with the workflow job event. For ephemeral runners the lambda requests a JIT (just in time) configuration via the GitHub API to register the runner. JIT configuration is limited to ephemeral runners (and currently not supported by GHES). For non-ephemeral runners, a registration token is always requested. In both cases the configuration is made available to the instance via the same SSM parameter. To disable JIT configuration for ephermeral runners set <code>enable_jit_config</code> to <code>false</code>. We also suggest using a pre-build AMI to improve the start time of jobs for ephemeral runners.</li> <li>GitHub Cloud vs GitHub Enterprise Server (GHES). The runners support GitHub Cloud as well GitHub Enterprise Server. For GHES, we rely on our community for support and testing. We at Philips have no capability to test GHES ourselves.</li> <li>Spot vs on-demand. The runners use either the EC2 spot or on-demand life cycle. Runners will be created via the AWS CreateFleet API. The module (scale up lambda) will request via the CreateFleet API to create instances in one of the subnets and of the specified instance types.</li> <li>ARM64 support via Graviton/Graviton2 instance-types. When using the default example or top-level module, specifying <code>instance_types</code> that match a Graviton/Graviton 2 (ARM64) architecture (e.g. a1, t4g or any 6th-gen <code>g</code> or <code>gd</code> type), you must also specify <code>runner_architecture = \"arm64\"</code> and the sub-modules will be automatically configured to provision with ARM64 AMIs and leverage GitHub's ARM64 action runner. See below for more details.</li> </ul>"},{"location":"configuration.html#aws-ssm-parameters","title":"AWS SSM Parameters","text":"<p>The module uses the AWS System Manager Parameter Store to store configuration for the runners, as well as registration tokens and secrets for the Lambdas. Paths for the parameters can be configured via the variable <code>ssm_paths</code>. The location of the configuration parameters is retrieved by the runners via the instance tag <code>ghr:ssm_config_path</code>. The following default paths will be used. Tokens or JIT config stored in the token path will be deleted after retrieval by instance, data not deleted after a day will be deleted by a SSM housekeeper lambda.</p> Path Description <code>ssm_paths.root/var.prefix?/app/</code> App secrets used by Lambda's <code>ssm_paths.root/var.prefix?/runners/config/&lt;name&gt;</code> Configuration parameters used by runner start script <code>ssm_paths.root/var.prefix?/runners/tokens/&lt;ec2-instance-id&gt;</code> Either JIT configuration (ephemeral runners) or registration tokens (non ephemeral runners) generated by the control plane (scale-up lambda), and consumed by the start script on the runner to activate / register the runner. <p>Available configuration parameters:</p> Parameter name Description <code>agent_mode</code> Indicates if the agent is running in ephemeral mode or not. <code>enable_cloudwatch</code> Configuration for the cloudwatch agent to stream logging. <code>run_as</code> The user used for running the GitHub action runner agent. <code>token_path</code> The path where tokens are stored."},{"location":"configuration.html#encryption","title":"Encryption","text":"<p>The module supports two scenarios to manage environment secrets and private keys of the Lambda functions.</p>"},{"location":"configuration.html#managed-kms-key-default","title":"Managed KMS key (default)","text":"<p>This is the default, no additional configuration is required.</p>"},{"location":"configuration.html#provided-kms-key","title":"Provided KMS key","text":"<p>You have to create and configure you KMS key. The module will use the context with key: <code>Environment</code> and value <code>var.environment</code> as encryption context.</p> <pre><code>resource \"aws_kms_key\" \"github\" {\n  is_enabled = true\n}\n\nmodule \"runners\" {\n\n  ...\n  kms_key_arn = aws_kms_key.github.arn\n  ...\n</code></pre>"},{"location":"configuration.html#pool","title":"Pool","text":"<p>The module supports two options for keeping a pool of runners. One is via a pool which only supports org-level runners, the second option is keeping runners idle.</p> <p>The pool is introduced in combination with the ephemeral runners and is primarily meant to ensure if any event is unexpectedly dropped and no runner was created, the pool can pick up the job. The pool is maintained by a lambda. Each time the lambda is triggered a check is performed to ensure the number of idle runners managed by the module matches the expected pool size. If not, the pool will be adjusted. Keep in mind that the scale down function is still active and will terminate instances that are detected as idle.</p> <pre><code>pool_runner_owner = \"my-org\"                  # Org to which the runners are added\npool_config = [{\n  size                = 20                    # size of the pool\n  schedule_expression = \"cron(* * * * ? *)\"   # cron expression to trigger the adjustment of the pool\n}]\n</code></pre> <p>The pool is NOT enabled by default and can be enabled by setting at least one object of the pool config list. The ephemeral example contains configuration options (commented out).</p>"},{"location":"configuration.html#idle-runners","title":"Idle runners","text":"<p>The module will scale down to zero runners by default. By specifying a <code>idle_config</code> config, idle runners can be kept active. The scale down lambda checks if any of the cron expressions matches the current time with a margin of 5 seconds. When there is a match, the number of runners specified in the idle config will be kept active. In case multiple cron expressions match, the first one will be used. Below is an idle configuration for keeping runners active from 9:00am to 5:59pm on working days. The cron expression generator by Cronhub is a great resource to set up your idle config.</p> <p>By default, the oldest instances are evicted. This helps keep your environment up-to-date and reduce problems like running out of disk space or RAM. Alternatively, if your older instances have a long-living cache, you can override the <code>evictionStrategy</code> to <code>newest_first</code> to evict the newest instances first instead.</p> <pre><code>idle_config = [{\n   cron             = \"* * 9-17 * * 1-5\"\n   timeZone         = \"Europe/Amsterdam\"\n   idleCount        = 2\n   # Defaults to 'oldest_first'\n   evictionStrategy = \"oldest_first\"\n}]\n</code></pre> <p>Note: When using Windows runners, we recommend keeping a few runners warmed up due to the minutes-long cold start time.</p>"},{"location":"configuration.html#supported-config","title":"Supported config","text":"<p>Cron expressions are parsed by cron-parser. The supported syntax.</p> <pre><code>*    *    *    *    *    *\n\u252c    \u252c    \u252c    \u252c    \u252c    \u252c\n\u2502    \u2502    \u2502    \u2502    \u2502    |\n\u2502    \u2502    \u2502    \u2502    \u2502    \u2514 day of week (0 - 7) (0 or 7 is Sun)\n\u2502    \u2502    \u2502    \u2502    \u2514\u2500\u2500\u2500\u2500\u2500 month (1 - 12)\n\u2502    \u2502    \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 day of month (1 - 31)\n\u2502    \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 hour (0 - 23)\n\u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 minute (0 - 59)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 second (0 - 59, optional)\n</code></pre> <p>For time zones please check TZ database name column for the supported values.</p>"},{"location":"configuration.html#ephemeral-runners","title":"Ephemeral runners","text":"<p>You can configure runners to be ephemeral, in which case runners will be used only for one job. The feature should be used in conjunction with listening for the workflow job event. Please consider the following:</p> <ul> <li>The scale down lambda is still active, and should only remove orphan instances. But there is no strict check in place. So ensure you configure the <code>minimum_running_time_in_minutes</code> to a value that is high enough to get your runner booted and connected to avoid it being terminated before executing a job.</li> <li>The messages sent from the webhook lambda to the scale-up lambda are by default delayed by SQS, to give available runners a chance to start the job before the decision is made to scale more runners. For ephemeral runners there is no need to wait. Set <code>delay_webhook_event</code> to <code>0</code>.</li> <li>All events in the queue will lead to a new runner created by the lambda. By setting <code>enable_job_queued_check</code> to <code>true</code> you can enforce a rule of only creating a runner if the event has a correlated queued job. Setting this can avoid creating useless runners. For example, a job getting cancelled before a runner was created or if the job was already picked up by another runner. We suggest using this in combination with a pool.</li> <li>To ensure runners are created in the same order GitHub sends the events, by default we use a FIFO queue. This is mainly relevant for repo level runners. For ephemeral runners you can set <code>enable_fifo_build_queue</code> to <code>false</code>.</li> <li>Errors related to scaling should be retried via SQS. You can configure <code>job_queue_retention_in_seconds</code> and <code>redrive_build_queue</code> to tune the behavior. We have no mechanism to avoid events never being processed, which means potentially no runner gets created and the job in GitHub times out in 6 hours.</li> </ul> <p>The example for ephemeral runners is based on the default example. Have look at the diff to see the major configuration differences.</p>"},{"location":"configuration.html#prebuilt-images","title":"Prebuilt Images","text":"<p>This module also allows you to run agents from a prebuilt AMI to gain faster startup times. The module provides several examples to build your own custom AMI. To remove old images, an AMI housekeeper module can be used. You can find more information in the image README.md for building custom images.</p>"},{"location":"configuration.html#logging","title":"Logging","text":"<p>The module uses AWS Lambda Powertools for logging. By default the log level is set to <code>info</code>, by setting the log level to <code>debug</code> the incoming events of the Lambda are logged as well.</p> <p>Log messages contains at least the following keys:</p> <ul> <li><code>messages</code>: The logged messages</li> <li><code>environment</code>: The environment prefix provided via Terraform</li> <li><code>service</code>: The lambda</li> <li><code>module</code>: The TypeScript module writing the log message</li> <li><code>function-name</code>: The name of the lambda function (prefix + function name)</li> <li><code>github</code>: Depending on the lambda, contains GitHub context</li> <li><code>runner</code>: Depending on the lambda, specific context related to the runner</li> </ul> <p>An example log message of the scale-up function:</p> <pre><code>{\n  \"level\": \"INFO\",\n  \"message\": \"Received event\",\n  \"service\": \"runners-scale-up\",\n  \"timestamp\": \"2023-03-20T08:15:27.448Z\",\n  \"xray_trace_id\": \"1-6418161e-08825c2f575213ef760531bf\",\n  \"module\": \"scale-up\",\n  \"region\": \"eu-west-1\",\n  \"environment\": \"my-linux-x64\",\n  \"aws-request-id\": \"eef1efb7-4c07-555f-9a67-b3255448ee60\",\n  \"function-name\": \"my-linux-x64-scale-up\",\n  \"runner\": {\n    \"type\": \"Repo\",\n    \"owner\": \"test-runners/multi-runner\"\n  },\n  \"github\": {\n    \"event\": \"workflow_job\",\n    \"workflow_job_id\": \"1234\"\n  }\n}\n</code></pre>"},{"location":"configuration.html#tracing","title":"Tracing","text":"<p>The distributed architecture of this application can make it difficult to troubleshoot. We support the option to enable tracing for all the lambda functions created by this application. To enable tracing, you can provide the <code>tracing_config</code> option inside the root module or inner modules.</p> <p>This tracing config generates timelines for following events:</p> <ul> <li>Basic lifecycle of lambda function</li> <li>Traces for Github API calls (can be configured by capture_http_requests).</li> <li>Traces for all AWS SDK calls</li> </ul> <p>This feature has been disabled by default.</p>"},{"location":"configuration.html#debugging","title":"Debugging","text":"<p>In case the setup does not work as intended, trace the events through this sequence:</p> <ul> <li>In the GitHub App configuration, the Advanced page displays all webhook events that were sent.</li> <li>In AWS CloudWatch, every lambda has a log group. Look at the logs of the <code>webhook</code> and <code>scale-up</code> lambdas.</li> <li>In AWS SQS you can see messages available or in flight.</li> <li>Once an EC2 instance is running, you can connect to it in the EC2 user interface using Session Manager (use <code>enable_ssm_on_runners = true</code>). Check the user data script using <code>cat /var/log/user-data.log</code>. By default several log files of the instances are streamed to AWS CloudWatch, look for a log group named <code>&lt;environment&gt;/runners</code>. In the log group you should see at least the log streams for the user data installation and runner agent.</li> <li>Registered instances should show up in the Settings - Actions page of the repository or organization (depending on the installation mode).</li> </ul>"},{"location":"configuration.html#experimental-features","title":"Experimental features","text":""},{"location":"configuration.html#queue-to-publish-workflow-job-events","title":"Queue to publish workflow job events","text":"<p>This queue is an experimental feature to allow you to receive a copy of the wokflow_jobs events sent by the GitHub App. This can be used to calculate a matrix or monitor the system.</p> <p>To enable the feature set <code>enable_workflow_job_events_queue = true</code>. Be aware though, this feature is experimental!</p> <p>Messages received on the queue are using the same format as published by GitHub wrapped in a property <code>workflowJobEvent</code>.</p> <pre><code>export interface GithubWorkflowEvent {\n  workflowJobEvent: WorkflowJobEvent;\n}\n</code></pre> <p>This extensible format allows more fields to be added if needed. You can configure the queue by setting properties to <code>workflow_job_events_queue_config</code></p> <p>NOTE: By default, a runner AMI update requires a re-apply of this terraform config (the runner AMI ID is looked up by a terraform data source). To avoid this, you can use <code>ami_id_ssm_parameter_name</code> to have the scale-up lambda dynamically lookup the runner AMI ID from an SSM parameter at instance launch time. Said SSM parameter is managed outside of this module (e.g. by a runner AMI build workflow).</p>"},{"location":"default.html","title":"Action runners deployment default example","text":"<p>This module shows how to create GitHub action runners. Lambda release will be downloaded from GitHub.</p>"},{"location":"default.html#usages","title":"Usages","text":"<p>Steps for the full setup, such as creating a GitHub app can be found in the root module's README. First download the Lambda releases from GitHub. Alternatively you can build the lambdas locally with Node or Docker, there is a simple build script in <code>&lt;root&gt;/.ci/build.sh</code>. In the <code>main.tf</code> you can simply remove the location of the lambda zip files, the default location will work in this case.</p> <p>Ensure you have set the version in <code>lambdas-download/main.tf</code> for running the example. The version needs to be set to a GitHub release version, see https://github.com/philips-labs/terraform-aws-github-runner/releases</p> <pre><code>cd ../lambdas-download\nterraform init\nterraform apply -var=module_version=&lt;VERSION&gt;\ncd -\n</code></pre> <p>Before running Terraform, ensure the GitHub app is configured. See the configuration details for more details.</p> <pre><code>terraform init\nterraform apply\n</code></pre> <p>The module will try to update the GitHub App webhook and secret (only linux/mac). You can receive the webhook details by running:</p> <pre><code>terraform output webhook_secret\n</code></pre>"},{"location":"default.html#requirements","title":"Requirements","text":"Name Version terraform &gt;= 1.3.0 aws ~&gt; 5.2 local ~&gt; 2.0 random ~&gt; 3.0"},{"location":"default.html#providers","title":"Providers","text":"Name Version random 3.5.1"},{"location":"default.html#modules","title":"Modules","text":"Name Source Version base ../base n/a runners ../../ n/a webhook_github_app ../../modules/webhook-github-app n/a"},{"location":"default.html#resources","title":"Resources","text":"Name Type random_id.random resource"},{"location":"default.html#inputs","title":"Inputs","text":"Name Description Type Default Required environment Environment name, used as prefix. <code>string</code> <code>null</code> no github_app GitHub for API usages. <pre>object({    id         = string    key_base64 = string  })</pre> n/a yes"},{"location":"default.html#outputs","title":"Outputs","text":"Name Description runners n/a webhook_endpoint n/a webhook_secret n/a"},{"location":"examples.html","title":"Examples","text":"<p>Examples are located in the examples directory. The following examples are provided:</p> <ul> <li>Default: The default example of the module</li> <li>ARM64: Example usage with ARM64 architecture</li> <li>Ephemeral: Example usages of ephemeral runners based on the default example.</li> <li>Multi Runner : Example usage of creating a multi runner which creates multiple runners/ configurations with a single deployment</li> <li>Permissions boundary: Example usages of permissions boundaries.</li> <li>Prebuilt Images: Example usages of deploying runners with a custom prebuilt image.</li> <li>Ubuntu: Example usage of creating a runner using Ubuntu AMIs.</li> <li>Windows: Example usage of creating a runner using Windows as the OS.</li> </ul>"},{"location":"examples.html#examples_1","title":"Examples","text":""},{"location":"security.html","title":"Security","text":"<p>This module creates resources in your AWS infrastructure, and EC2 instances for hosting the self-hosted runners on-demand. IAM permissions are set to a minimal level, and could be further limited by using permission boundaries. Instances permissions are limited to retrieve and delete the registration token, access the instance's own tags, and terminate the instance itself. By nature instances are short-lived, we strongly suggest to use ephemeral runners to ensure a safe build environment for each workflow job execution.</p> <p>Ephemeral runners are using the JIT configuration, confguration that only can be used once to activate a runner. For non-ephemeral runners this option is not provided by GitHub. For non-ephemeeral runners a registration token is passed via SSM. After using the token, the token is deleted. But the token remains valid and is potential available in memory on the runner. For ephemeral runners this problem is avoid by using just in time tokens.</p> <p>The examples are using standard AMI's for different operation systems. Instances are not hardened, and sudo operation are not blocked. To provide an out of the box working experience by default the module installs and configures the runner. However secrets are not hard coded, they finally end up in the memory of the instances. You can harden the instance by providing your own AMI and overwriting the cloud-init script.</p> <p>We welcome any improvement to the standard module to make the default as secure as possible, in the end it remains your responsibility to keep your environment secure.</p>"},{"location":"security.html#terraform-module-for-scalable-self-hosted-github-action-runners","title":"Terraform module for scalable self hosted GitHub action runners","text":"<p>This Terraform module creates the required infrastructure needed to host GitHub Actions self-hosted, auto-scaling runners on AWS spot instances. It provides the required logic to handle the life cycle for scaling up and down using a set of AWS Lambda functions. Runners are scaled down to zero to avoid costs when no workflows are active.</p> <p>\ud83d\udce2 We maintain the project as a truly open-source project. We maintain the project on a best effort basis. We welcome contributions from the community. Feel free to help us answering issues, reviewing PRs, or maintaining and improving the project.</p> <p>\ud83d\udce2 <code>v5</code> replaces Amazon Linux 2 with Amazon Linux 2023 as default OS. Check the PR for more details and other changes.</p> <p>\ud83d\udce2 For contibutions to older versions you can make a PR to the related branch, e.g. <code>v4</code>. We have no release process in place for older versions.</p> <p>\ud83d\udce2 HELP WANTED: We have been running the AWS self-hosted GitHub runners OS project in Philips Labs for over two years! And we are incredibly happy with all the feedback and contributions of the open-source community. In the next months we will speak at some conferences to share the solution and story of running this open-source project. Via this questionnaire we would like to gather  feedback from the community to use in our talks.</p> <ul> <li>Motivation</li> <li>Overview</li> <li>Major configuration options</li> <li>AWS SSM Parameters</li> <li>Usages</li> <li>Setup GitHub App (part 1)</li> <li>Setup terraform module</li> <li>Setup the webhook / GitHub App (part 2)<ul> <li>Option 1: Webhook</li> <li>Option 2: App</li> <li>Install app</li> </ul> </li> <li>Encryption</li> <li>Pool</li> <li>Idle runners</li> <li>Ephemeral runners</li> <li>Prebuilt Images</li> <li>Experimental - Optional queue to publish GitHub workflow job events</li> <li>Examples</li> <li>Sub modules</li> <li>Logging</li> <li>Tracing</li> <li>Debugging</li> <li>Security Considerations</li> <li>Requirements</li> <li>Providers</li> <li>Modules</li> <li>Resources</li> <li>Inputs</li> <li>Outputs</li> <li>Contributing</li> <li>Philips Forest</li> </ul>"},{"location":"security.html#motivation","title":"Motivation","text":"<p>GitHub Actions <code>self-hosted</code> runners provide a flexible option to run CI workloads on the infrastructure of your choice. However, currently GitHub does not provide tooling to automate the creation and scaling of action runners. This module creates the AWS infrastructure to host action runners on spot instances. It also provides lambda modules to orchestrate the life cycle of the action runners.</p> <p>Lambda was selected as the preferred runtime for two primary reasons. Firstly, it enables the development of compact components with limited access to AWS and GitHub. Secondly, it offers a scalable configuration with minimal expenses, applicable at both the repository and organizational levels. The Lambda functions will be responsible for provisioning Linux-based EC2 instances equipped with Docker to handle CI workloads compatible with Linux and/or Docker. The primary objective is to facilitate Docker-based workloads.</p> <p>A pertinent question may arise: why not opt for Kubernetes? The current strategy aligns closely with the implementation of GitHub's action runners. The chosen approach involves installing the runner on a host where the necessary software is readily available, maintaining proximity to GitHub's existing practices. Another viable option could be AWS Auto Scaling groups. However, this alternative usually demands broader permissions at the instance level from GitHub. Additionally, managing the scaling process, both up and down, becomes a non-trivial task in this scenario.</p>"},{"location":"security.html#overview","title":"Overview","text":"<p>The moment a GitHub action workflow requiring a <code>self-hosted</code> runner is triggered, GitHub will try to find a runner which can execute the workload. See additional notes for how the selection is made. This module reacts to GitHub's <code>workflow_job</code> event for the triggered workflow and creates a new runner if necessary.</p> <p>For receiving the <code>workflow_job</code> event by the webhook (lambda), a webhook needs to be created in GitHub. The <code>check_run</code> option was dropped from version 2.x. The following options to send the event are supported.</p> <ul> <li>Create a GitHub app, define a webhook and subscribe the app to the <code>workflow_job</code> event.</li> <li>Create a webhook on enterprise, org or repo level, define a webhook and subscribe the app to the <code>workflow_job</code> event.</li> </ul> <p>In AWS an API gateway endpoint is created that is able to receive the GitHub webhook events via HTTP post. The gateway triggers the webhook lambda which will verify the signature of the event. This check guarantees the event is sent by the GitHub App. The lambda only handles <code>workflow_job</code> events with status <code>queued</code> and matching the runner labels. The accepted events are posted on a SQS queue. Messages on this queue will be delayed for a configurable amount of seconds (default 30 seconds) to give the available runners time to pick up this build.</p> <p>The \"Scale Up Runner\" Lambda actively monitors the SQS queue, processing incoming events. The Lambda conducts a series of checks to determine the necessity of creating a new EC2 spot instance. For instance, it refrains from creating an instance if a build is already initiated by an existing runner or if the maximum allowable number of runners has been reached.</p> <p>The Lambda first requests a JIT configuration or registration token from GitHub, which is needed later by the runner to register itself. This avoids the case that the EC2 instance, which later in the process will install the agent, needs administration permissions to register the runner. Next, the EC2 spot instance is created via the launch template. The launch template defines the specifications of the required instance and contains a <code>user_data</code> script. This script will install the required software and configure it. The registration token for the action runner is stored in the parameter store (SSM), from which the user data script will fetch it and delete it once it has been retrieved. Once the user data script is finished, the action runner should be online, and the workflow will start in seconds.</p> <p>The current method for scaling down runners employs a straightforward approach: at predefined intervals, the Lambda conducts a thorough examination of each runner (instance) to assess its activity. If a runner is found to be idle, it is deregistered from GitHub, and the associated AWS instance is terminated. Presently, no alternative method appears available for achieving a more gradual scaling down process.</p> <p>To address potential delays in downloading the GitHub Action Runner distribution, a lambda function has been implemented to synchronize the action runner binary from GitHub to an S3 bucket. This ensures that the EC2 instance can retrieve the distribution from the S3 bucket, mitigating the need to rely on internet downloads, which can occasionally take more than 10 minutes.</p> <p>Sensitive information such as secrets and private keys is stored securely in the SSM Parameter Store. These values undergo encryption using either the default KMS key for SSM or a custom KMS key, depending on the specified configuration.</p> <p></p> <p>Permission are managed in several places. Below are the most important ones. For details check the Terraform sources.</p> <ul> <li>The GitHub App requires access to actions and to publish <code>workflow_job</code> events to the AWS webhook (API gateway).</li> <li>The scale up lambda should have access to EC2 for creating and tagging instances.</li> <li>The scale down lambda should have access to EC2 to terminate instances.</li> </ul> <p>Besides these permissions, the lambdas also need permission to CloudWatch (for logging and scheduling), SSM and S3. For more details about the required permissions see the documentation of the IAM module which uses permission boundaries.</p>"},{"location":"security.html#major-configuration-options","title":"Major configuration options","text":"<p>To be able to support a number of use-cases, the module has quite a lot of configuration options. We tried to choose reasonable defaults. Several examples also show the main cases of how to configure the runners.</p> <ul> <li>Org vs Repo level. You can configure the module to connect the runners in GitHub on an org level and share the runners in your org, or set the runners on repo level and the module will install the runner to the repo. There can be multiple repos but runners are not shared between repos.</li> <li>Multi-Runner module. This modules allows you to create multiple runner configurations with a single webhook and single GitHub App to simplify deployment of different types of runners. Refer to the ReadMe for more information to understand the functionality.</li> <li>Workflow job event. You can configure the webhook in GitHub to send workflow job events to the webhook. Workflow job events were introduced by GitHub in September 2021 and are designed to support scalable runners. We advise using the workflow job event when possible.</li> <li>Linux vs Windows. You can configure the OS types linux and win. Linux will be used by default.</li> <li>Re-use vs Ephemeral. By default runners are re-used, until detected idle. Once idle they will be removed from the pool. To improve security we are introducing ephemeral runners. Those runners are only used for one job. Ephemeral runners only work in combination with the workflow job event. For ephemeral runners the lambda requests a JIT (just in time) configuration via the GitHub API to register the runner. JIT configuration is limited to ephemeral runners (and currently not supported by GHES). For non-ephemeral runners, a registration token is always requested. In both cases the configuration is made available to the instance via the same SSM parameter. To disable JIT configuration for ephermeral runners set <code>enable_jit_config</code> to <code>false</code>. We also suggest using a pre-build AMI to improve the start time of jobs for ephemeral runners.</li> <li>GitHub Cloud vs GitHub Enterprise Server (GHES). The runners support GitHub Cloud as well GitHub Enterprise Server. For GHES, we rely on our community for support and testing. We at Philips have no capability to test GHES ourselves.</li> <li>Spot vs on-demand. The runners use either the EC2 spot or on-demand life cycle. Runners will be created via the AWS CreateFleet API. The module (scale up lambda) will request via the CreateFleet API to create instances in one of the subnets and of the specified instance types.</li> <li>ARM64 support via Graviton/Graviton2 instance-types. When using the default example or top-level module, specifying <code>instance_types</code> that match a Graviton/Graviton 2 (ARM64) architecture (e.g. a1, t4g or any 6th-gen <code>g</code> or <code>gd</code> type), you must also specify <code>runner_architecture = \"arm64\"</code> and the sub-modules will be automatically configured to provision with ARM64 AMIs and leverage GitHub's ARM64 action runner. See below for more details.</li> </ul>"},{"location":"security.html#aws-ssm-parameters","title":"AWS SSM Parameters","text":"<p>The module uses the AWS System Manager Parameter Store to store configuration for the runners, as well as registration tokens and secrets for the Lambdas. Paths for the parameters can be configured via the variable <code>ssm_paths</code>. The location of the configuration parameters is retrieved by the runners via the instance tag <code>ghr:ssm_config_path</code>. The following default paths will be used. Tokens or JIT config stored in the token path will be deleted after retrieval by instance, data not deleted after a day will be deleted by a SSM housekeeper lambda.</p> Path Description <code>ssm_paths.root/var.prefix?/app/</code> App secrets used by Lambda's <code>ssm_paths.root/var.prefix?/runners/config/&lt;name&gt;</code> Configuration parameters used by runner start script <code>ssm_paths.root/var.prefix?/runners/tokens/&lt;ec2-instance-id&gt;</code> Either JIT configuration (ephemeral runners) or registration tokens (non ephemeral runners) generated by the control plane (scale-up lambda), and consumed by the start script on the runner to activate / register the runner. <p>Available configuration parameters:</p> Parameter name Description <code>agent_mode</code> Indicates if the agent is running in ephemeral mode or not. <code>enable_cloudwatch</code> Configuration for the cloudwatch agent to stream logging. <code>run_as</code> The user used for running the GitHub action runner agent. <code>token_path</code> The path where tokens are stored."},{"location":"security.html#usages","title":"Usages","text":"<p>Examples are provided in the example directory. Please ensure you have installed the following tools.</p> <ul> <li>Terraform, or tfenv.</li> <li>Bash shell or compatible</li> <li>Docker (optional, to build lambdas without node).</li> <li>AWS cli (optional)</li> <li>Node and yarn (for lambda development).</li> </ul> <p>The module supports two main scenarios for creating runners. Repository level runners will be dedicated to only one repository, so no other repository can use the runner. At the organization level, you can use the runner(s) for all repositories within the organization. See GitHub self-hosted runner instructions for more information. Before starting the deployment you have to choose one option.</p> <p>The setup consists of running Terraform to create all AWS resources and manually configuring the GitHub App. The Terraform module requires configuration from the GitHub App and the GitHub app requires output from Terraform. Therefore you first create the GitHub App and configure the basics, then run Terraform, and afterwards finalize the configuration of the GitHub App.</p>"},{"location":"security.html#setup-github-app-part-1","title":"Setup GitHub App (part 1)","text":"<p>Go to GitHub and create a new app. Be aware you can create apps for your organization or for a user. For now we only support organization level apps.</p> <ol> <li>Create an app in Github</li> <li>Choose a name</li> <li>Choose a website (mandatory, not required for the module).</li> <li>Disable the webhook for now (we will configure this later or create an alternative webhook).</li> <li>Permissions for all runners:<ul> <li>Repository:</li> <li><code>Actions</code>: Read-only (check for queued jobs)</li> <li><code>Checks</code>: Read-only (receive events for new builds)</li> <li><code>Metadata</code>: Read-only (default/required)</li> </ul> </li> <li>Permissions for repo level runners only:</li> <li>Repository:<ul> <li><code>Administration</code>: Read &amp; write (to register runner)</li> </ul> </li> <li>Permissions for organization level runners only:</li> <li>Organization<ul> <li><code>Self-hosted runners</code>: Read &amp; write (to register runner)</li> </ul> </li> <li>Save the new app.</li> <li>On the General page, make a note of the \"App ID\" and \"Client ID\" parameters.</li> <li>Generate a new private key and save the <code>app.private-key.pem</code> file.</li> </ol>"},{"location":"security.html#setup-terraform-module","title":"Setup terraform module","text":""},{"location":"security.html#download-lambdas","title":"Download lambdas","text":"<p>To apply the terraform module, the compiled lambdas (.zip files) need to be available either locally or in an S3 bucket. They can either be downloaded from the GitHub release page or built locally.</p> <p>To read the files from S3, set the <code>lambda_s3_bucket</code> variable and the specific object key for each lambda.</p> <p>The lambdas can be downloaded manually from the release page or using the download-lambda terraform module (requires <code>curl</code> to be installed on your machine). In the <code>download-lambda</code> directory, run <code>terraform init &amp;&amp; terraform apply</code>. The lambdas will be saved to the same directory.</p> <p>For local development you can build all the lambdas at once using <code>.ci/build.sh</code> or individually using <code>yarn dist</code>.</p>"},{"location":"security.html#service-linked-role","title":"Service-linked role","text":"<p>To create spot instances the <code>AWSServiceRoleForEC2Spot</code> role needs to be added to your account. You can do that manually by following the AWS docs. To use terraform for creating the role, either add the following resource or let the module manage the service linked role by setting <code>create_service_linked_role_spot</code> to <code>true</code>. Be aware this is an account global role, so maybe you don't want to manage it via a specific deployment.</p> <pre><code>resource \"aws_iam_service_linked_role\" \"spot\" {\n  aws_service_name = \"spot.amazonaws.com\"\n}\n</code></pre>"},{"location":"security.html#terraform-module","title":"Terraform module","text":"<p>Next create a second terraform workspace and initiate the module, or adapt one of the examples.</p> <p>Note that <code>github_app.key_base64</code> needs to be a base64-encoded string of the <code>.pem</code> file i.e. the output of <code>base64 app.private-key.pem</code>. The decoded string can either be a multiline value or a single line value with new lines represented with literal <code>\\n</code> characters.</p> <pre><code>module \"github-runner\" {\n  source  = \"philips-labs/github-runner/aws\"\n  version = \"REPLACE_WITH_VERSION\"\n\n  aws_region = \"eu-west-1\"\n  vpc_id     = \"vpc-123\"\n  subnet_ids = [\"subnet-123\", \"subnet-456\"]\n\n  prefix = \"gh-ci\"\n\n  github_app = {\n    key_base64     = \"base64string\"\n    id             = \"1\"\n    webhook_secret = \"webhook_secret\"\n  }\n\n  webhook_lambda_zip                = \"lambdas-download/webhook.zip\"\n  runner_binaries_syncer_lambda_zip = \"lambdas-download/runner-binaries-syncer.zip\"\n  runners_lambda_zip                = \"lambdas-download/runners.zip\"\n  enable_organization_runners = true\n}\n</code></pre> <p>Run terraform by using the following commands</p> <pre><code>terraform init\nterraform apply\n</code></pre> <p>The terraform output displays the API gateway url (endpoint) and secret, which you need in the next step.</p> <p>The lambda for syncing the GitHub distribution to S3 is triggered via CloudWatch (by default once per hour). After deployment the function is triggered via S3 to ensure the distribution is cached.</p>"},{"location":"security.html#setup-the-webhook-github-app-part-2","title":"Setup the webhook / GitHub App (part 2)","text":"<p>At this point you have two options. Either create a separate webhook (enterprise, org, or repo), or create a webhook in the App.</p>"},{"location":"security.html#option-1-webhook","title":"Option 1: Webhook","text":"<ol> <li>Create a new webhook at the repo level for repo level runners, or org (or enterprise level) for org level runners.</li> <li>Provide the webhook url, which should be part of the output of terraform.</li> <li>Provide the webhook secret (<code>terraform output -raw &lt;NAME_OUTPUT_VAR&gt;</code>).</li> <li>Ensure the content type is <code>application/json</code>.</li> <li>In the \"Permissions &amp; Events\" section and then \"Subscribe to Events\" subsection, check either \"Workflow Job\" or \"Check Run\" (choose only one option!!!).</li> <li>In the \"Install App\" section, install the App in your organization, either in all or in selected repositories.</li> </ol>"},{"location":"security.html#option-2-app","title":"Option 2: App","text":"<p>Go back to the GitHub App and update the following settings.</p> <ol> <li>Enable the webhook.</li> <li>Provide the webhook url, should be part of the output of terraform.</li> <li>Provide the webhook secret (<code>terraform output -raw &lt;NAME_OUTPUT_VAR&gt;</code>).</li> <li>In the \"Permissions &amp; Events\" section and then \"Subscribe to Events\" subsection, check either \"Workflow Job\" or \"Check Run\" (choose only one option!!!).</li> </ol>"},{"location":"security.html#install-app","title":"Install app","text":"<p>Finally you need to ensure the app is installed to all or selected repositories.</p> <p>Go back to the GitHub App and update the following settings.</p> <ol> <li>In the \"Install App\" section, install the App in your organization, either in all or in selected repositories.</li> </ol>"},{"location":"security.html#encryption","title":"Encryption","text":"<p>The module supports two scenarios to manage environment secrets and private keys of the Lambda functions.</p>"},{"location":"security.html#encrypted-via-a-module-managed-kms-key-default","title":"Encrypted via a module managed KMS key (default)","text":"<p>This is the default, no additional configuration is required.</p>"},{"location":"security.html#encrypted-via-a-provided-kms-key","title":"Encrypted via a provided KMS key","text":"<p>You have to create and configure you KMS key. The module will use the context with key: <code>Environment</code> and value <code>var.environment</code> as encryption context.</p> <pre><code>resource \"aws_kms_key\" \"github\" {\n  is_enabled = true\n}\n\nmodule \"runners\" {\n\n  ...\n  kms_key_arn = aws_kms_key.github.arn\n  ...\n</code></pre>"},{"location":"security.html#pool","title":"Pool","text":"<p>The module supports two options for keeping a pool of runners. One is via a pool which only supports org-level runners, the second option is keeping runners idle.</p> <p>The pool is introduced in combination with the ephemeral runners and is primarily meant to ensure if any event is unexpectedly dropped and no runner was created, the pool can pick up the job. The pool is maintained by a lambda. Each time the lambda is triggered a check is performed to ensure the number of idle runners managed by the module matches the expected pool size. If not, the pool will be adjusted. Keep in mind that the scale down function is still active and will terminate instances that are detected as idle.</p> <pre><code>pool_runner_owner = \"my-org\"                  # Org to which the runners are added\npool_config = [{\n  size                = 20                    # size of the pool\n  schedule_expression = \"cron(* * * * ? *)\"   # cron expression to trigger the adjustment of the pool\n}]\n</code></pre> <p>The pool is NOT enabled by default and can be enabled by setting at least one object of the pool config list. The ephemeral example contains configuration options (commented out).</p>"},{"location":"security.html#idle-runners","title":"Idle runners","text":"<p>The module will scale down to zero runners by default. By specifying a <code>idle_config</code> config, idle runners can be kept active. The scale down lambda checks if any of the cron expressions matches the current time with a margin of 5 seconds. When there is a match, the number of runners specified in the idle config will be kept active. In case multiple cron expressions match, the first one will be used. Below is an idle configuration for keeping runners active from 9:00am to 5:59pm on working days. The cron expression generator by Cronhub is a great resource to set up your idle config.</p> <p>By default, the oldest instances are evicted. This helps keep your environment up-to-date and reduce problems like running out of disk space or RAM. Alternatively, if your older instances have a long-living cache, you can override the <code>evictionStrategy</code> to <code>newest_first</code> to evict the newest instances first instead.</p> <pre><code>idle_config = [{\n   cron             = \"* * 9-17 * * 1-5\"\n   timeZone         = \"Europe/Amsterdam\"\n   idleCount        = 2\n   # Defaults to 'oldest_first'\n   evictionStrategy = \"oldest_first\"\n}]\n</code></pre> <p>Note: When using Windows runners, we recommend keeping a few runners warmed up due to the minutes-long cold start time.</p>"},{"location":"security.html#supported-config","title":"Supported config","text":"<p>Cron expressions are parsed by cron-parser. The supported syntax.</p> <pre><code>*    *    *    *    *    *\n\u252c    \u252c    \u252c    \u252c    \u252c    \u252c\n\u2502    \u2502    \u2502    \u2502    \u2502    |\n\u2502    \u2502    \u2502    \u2502    \u2502    \u2514 day of week (0 - 7) (0 or 7 is Sun)\n\u2502    \u2502    \u2502    \u2502    \u2514\u2500\u2500\u2500\u2500\u2500 month (1 - 12)\n\u2502    \u2502    \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 day of month (1 - 31)\n\u2502    \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 hour (0 - 23)\n\u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 minute (0 - 59)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 second (0 - 59, optional)\n</code></pre> <p>For time zones please check TZ database name column for the supported values.</p>"},{"location":"security.html#ephemeral-runners","title":"Ephemeral runners","text":"<p>You can configure runners to be ephemeral, in which case runners will be used only for one job. The feature should be used in conjunction with listening for the workflow job event. Please consider the following:</p> <ul> <li>The scale down lambda is still active, and should only remove orphan instances. But there is no strict check in place. So ensure you configure the <code>minimum_running_time_in_minutes</code> to a value that is high enough to get your runner booted and connected to avoid it being terminated before executing a job.</li> <li>The messages sent from the webhook lambda to the scale-up lambda are by default delayed by SQS, to give available runners a chance to start the job before the decision is made to scale more runners. For ephemeral runners there is no need to wait. Set <code>delay_webhook_event</code> to <code>0</code>.</li> <li>All events in the queue will lead to a new runner created by the lambda. By setting <code>enable_job_queued_check</code> to <code>true</code> you can enforce a rule of only creating a runner if the event has a correlated queued job. Setting this can avoid creating useless runners. For example, a job getting cancelled before a runner was created or if the job was already picked up by another runner. We suggest using this in combination with a pool.</li> <li>To ensure runners are created in the same order GitHub sends the events, by default we use a FIFO queue. This is mainly relevant for repo level runners. For ephemeral runners you can set <code>enable_fifo_build_queue</code> to <code>false</code>.</li> <li>Errors related to scaling should be retried via SQS. You can configure <code>job_queue_retention_in_seconds</code> and <code>redrive_build_queue</code> to tune the behavior. We have no mechanism to avoid events never being processed, which means potentially no runner gets created and the job in GitHub times out in 6 hours.</li> </ul> <p>The example for ephemeral runners is based on the default example. Have look at the diff to see the major configuration differences.</p>"},{"location":"security.html#prebuilt-images","title":"Prebuilt Images","text":"<p>This module also allows you to run agents from a prebuilt AMI to gain faster startup times. The module provides several examples to build your own custom AMI. To remove old images, an AMI housekeeper module can be used. You can find more information in the image README.md for building custom images.</p>"},{"location":"security.html#experimental-optional-queue-to-publish-github-workflow-job-events","title":"Experimental - Optional queue to publish GitHub workflow job events","text":"<p>This queue is an experimental feature to allow you to receive a copy of the wokflow_jobs events sent by the GitHub App. This can be used to calculate a matrix or monitor the system.</p> <p>To enable the feature set <code>enable_workflow_job_events_queue = true</code>. Be aware though, this feature is experimental!</p> <p>Messages received on the queue are using the same format as published by GitHub wrapped in a property <code>workflowJobEvent</code>.</p> <pre><code>export interface GithubWorkflowEvent {\n  workflowJobEvent: WorkflowJobEvent;\n}\n</code></pre> <p>This extensible format allows more fields to be added if needed. You can configure the queue by setting properties to <code>workflow_job_events_queue_config</code></p> <p>NOTE: By default, a runner AMI update requires a re-apply of this terraform config (the runner AMI ID is looked up by a terraform data source). To avoid this, you can use <code>ami_id_ssm_parameter_name</code> to have the scale-up lambda dynamically lookup the runner AMI ID from an SSM parameter at instance launch time. Said SSM parameter is managed outside of this module (e.g. by a runner AMI build workflow).</p>"},{"location":"security.html#examples","title":"Examples","text":"<p>Examples are located in the examples directory. The following examples are provided:</p> <ul> <li>Default: The default example of the module</li> <li>ARM64: Example usage with ARM64 architecture</li> <li>Ephemeral: Example usages of ephemeral runners based on the default example.</li> <li>Multi Runner : Example usage of creating a multi runner which creates multiple runners/ configurations with a single deployment</li> <li>Permissions boundary: Example usages of permissions boundaries.</li> <li>Prebuilt Images: Example usages of deploying runners with a custom prebuilt image.</li> <li>Ubuntu: Example usage of creating a runner using Ubuntu AMIs.</li> <li>Windows: Example usage of creating a runner using Windows as the OS.</li> </ul>"},{"location":"security.html#sub-modules","title":"Sub modules","text":"<p>The module contains several submodules, you can use the module via the main module or assemble your own setup by initializing the submodules yourself.</p> <p>The following submodules are the core of the module and are mandatory:</p> <ul> <li>runner-binaries-syncer - Syncs the action runner distribution.</li> <li>runners - Scales the action runners up and down</li> <li>webhook - Handles GitHub webhooks</li> <li>multi-runner - Creates multiple runner configurations in a single deployment</li> </ul> <p>The following sub modules are optional and are provided as examples or utilities:</p> <ul> <li>download-lambda - Utility module to download lambda artifacts from GitHub Release</li> <li>setup-iam-permissions - Example module to setup permission boundaries</li> </ul> <p>ARM64 configuration for submodules. When using the top level module configure <code>runner_architecture = \"arm64\"</code> and ensure the list of <code>instance_types</code> matches. When not using the top-level, ensure these properties are set on the submodules.</p>"},{"location":"security.html#logging","title":"Logging","text":"<p>The module uses AWS Lambda Powertools for logging. By default the log level is set to <code>info</code>, by setting the log level to <code>debug</code> the incoming events of the Lambda are logged as well.</p> <p>Log messages contains at least the following keys:</p> <ul> <li><code>messages</code>: The logged messages</li> <li><code>environment</code>: The environment prefix provided via Terraform</li> <li><code>service</code>: The lambda</li> <li><code>module</code>: The TypeScript module writing the log message</li> <li><code>function-name</code>: The name of the lambda function (prefix + function name)</li> <li><code>github</code>: Depending on the lambda, contains GitHub context</li> <li><code>runner</code>: Depending on the lambda, specific context related to the runner</li> </ul> <p>An example log message of the scale-up function:</p> <pre><code>{\n    \"level\": \"INFO\",\n    \"message\": \"Received event\",\n    \"service\": \"runners-scale-up\",\n    \"timestamp\": \"2023-03-20T08:15:27.448Z\",\n    \"xray_trace_id\": \"1-6418161e-08825c2f575213ef760531bf\",\n    \"module\": \"scale-up\",\n    \"region\": \"eu-west-1\",\n    \"environment\": \"my-linux-x64\",\n    \"aws-request-id\": \"eef1efb7-4c07-555f-9a67-b3255448ee60\",\n    \"function-name\": \"my-linux-x64-scale-up\",\n    \"runner\": {\n        \"type\": \"Repo\",\n        \"owner\": \"test-runners/multi-runner\"\n    },\n    \"github\": {\n        \"event\": \"workflow_job\",\n        \"workflow_job_id\": \"1234\"\n    }\n}\n</code></pre>"},{"location":"security.html#tracing","title":"Tracing","text":"<p>The distributed architecture of this application can make it difficult to troubleshoot.  We support the option to enable tracing for all the lambda functions created by this application. To enable tracing, you can provide the <code>tracing_config</code> option inside the root module or inner modules.</p> <p>This tracing config generates timelines for following events: - Basic lifecycle of lambda function - Traces for Github API calls (can be configured by capture_http_requests). - Traces for all AWS SDK calls</p> <p>This feature has been disabled by default.</p>"},{"location":"security.html#debugging","title":"Debugging","text":"<p>In case the setup does not work as intended, trace the events through this sequence:</p> <ul> <li>In the GitHub App configuration, the Advanced page displays all webhook events that were sent.</li> <li>In AWS CloudWatch, every lambda has a log group. Look at the logs of the <code>webhook</code> and <code>scale-up</code> lambdas.</li> <li>In AWS SQS you can see messages available or in flight.</li> <li>Once an EC2 instance is running, you can connect to it in the EC2 user interface using Session Manager (use <code>enable_ssm_on_runners = true</code>). Check the user data script using <code>cat /var/log/user-data.log</code>. By default several log files of the instances are streamed to AWS CloudWatch, look for a log group named <code>&lt;environment&gt;/runners</code>. In the log group you should see at least the log streams for the user data installation and runner agent.</li> <li>Registered instances should show up in the Settings - Actions page of the repository or organization (depending on the installation mode).</li> </ul>"},{"location":"security.html#security-considerations","title":"Security Considerations","text":"<p>This module creates resources in your AWS infrastructure, and EC2 instances for hosting the self-hosted runners on-demand. IAM permissions are set to a minimal level, and could be further limited by using permission boundaries. Instances permissions are limited to retrieve and delete the registration token, access the instance's own tags, and terminate the instance itself. By nature, instances are short-lived, and we strongly suggest using ephemeral runners to ensure a safe build environment for each workflow job execution.</p> <p>Ephemeral runners use the JIT configuration, which can be used only once to activate a runner. For non-ephemeral runners this option is not provided by GitHub, so instead a registration token is passed via SSM. After using the token, the token is deleted. But the token remains valid and is potential available in memory on the runner. For ephemeral runners this problem is avoid by using just-in-time tokens.</p> <p>The examples are using standard AMI's for different operation systems. Instances are not hardened, and sudo operation are not blocked. To provide an out of the box working experience by default the module installs and configures the runner. However secrets are not hard coded, they finally end up in the memory of the instances. You can harden the instance by providing your own AMI and overwriting the cloud-init script.</p> <p>We welcome any improvement to the standard module to make the default as secure as possible. But in the end it remains your responsibility to keep your environment secure.</p> Terraform root module documention  ## Requirements  | Name | Version | |------|---------| |  [terraform](#requirement\\_terraform) | &gt;= 1.3.0 | |  [aws](#requirement\\_aws) | ~&gt; 5.2 | |  [random](#requirement\\_random) | ~&gt; 3.0 |  ## Providers  | Name | Version | |------|---------| |  [aws](#provider\\_aws) | ~&gt; 5.2 | |  [random](#provider\\_random) | ~&gt; 3.0 |  ## Modules  | Name | Source | Version | |------|--------|---------| |  [ami\\_housekeeper](#module\\_ami\\_housekeeper) | ./modules/ami-housekeeper | n/a | |  [runner\\_binaries](#module\\_runner\\_binaries) | ./modules/runner-binaries-syncer | n/a | |  [runners](#module\\_runners) | ./modules/runners | n/a | |  [ssm](#module\\_ssm) | ./modules/ssm | n/a | |  [webhook](#module\\_webhook) | ./modules/webhook | n/a |  ## Resources  | Name | Type | |------|------| | [aws_sqs_queue.queued_builds](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/sqs_queue) | resource | | [aws_sqs_queue.queued_builds_dlq](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/sqs_queue) | resource | | [aws_sqs_queue.webhook_events_workflow_job_queue](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/sqs_queue) | resource | | [aws_sqs_queue_policy.build_queue_dlq_policy](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/sqs_queue_policy) | resource | | [aws_sqs_queue_policy.build_queue_policy](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/sqs_queue_policy) | resource | | [aws_sqs_queue_policy.webhook_events_workflow_job_queue_policy](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/sqs_queue_policy) | resource | | [random_string.random](https://registry.terraform.io/providers/hashicorp/random/latest/docs/resources/string) | resource | | [aws_iam_policy_document.deny_unsecure_transport](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/iam_policy_document) | data source |  ## Inputs  | Name | Description | Type | Default | Required | |------|-------------|------|---------|:--------:| |  [ami\\_filter](#input\\_ami\\_filter) | Map of lists used to create the AMI filter for the action runner AMI. | `map(list(string))` | <pre>{  \"state\": [    \"available\"  ]}</pre> | no | |  [ami\\_housekeeper\\_cleanup\\_config](#input\\_ami\\_housekeeper\\_cleanup\\_config) | Configuration for AMI cleanup.    `amiFilters` - Filters to use when searching for AMIs to cleanup. Default filter for images owned by the account and that are available.    `dryRun` - If true, no AMIs will be deregistered. Default false.    `launchTemplateNames` - Launch template names to use when searching for AMIs to cleanup. Default no launch templates.    `maxItems` - The maximum numer of AMI's tha will be queried for cleanup. Default no maximum.    `minimumDaysOld` - Minimum number of days old an AMI must be to be considered for cleanup. Default 30.    `ssmParameterNames` - SSM parameter names to use when searching for AMIs to cleanup. This parameter should be set when using SSM to configure the AMI to use. Default no SSM parameters. | <pre>object({    amiFilters = optional(list(object({      Name   = string      Values = list(string)      })),      [{        Name : \"state\",        Values : [\"available\"],        },        {          Name : \"image-type\",          Values : [\"machine\"],      }]    )    dryRun              = optional(bool, false)    launchTemplateNames = optional(list(string))    maxItems            = optional(number)    minimumDaysOld      = optional(number, 30)    ssmParameterNames   = optional(list(string))  })</pre> | `{}` | no | |  [ami\\_housekeeper\\_lambda\\_s3\\_key](#input\\_ami\\_housekeeper\\_lambda\\_s3\\_key) | S3 key for syncer lambda function. Required if using S3 bucket to specify lambdas. | `string` | `null` | no | |  [ami\\_housekeeper\\_lambda\\_s3\\_object\\_version](#input\\_ami\\_housekeeper\\_lambda\\_s3\\_object\\_version) | S3 object version for syncer lambda function. Useful if S3 versioning is enabled on source bucket. | `string` | `null` | no | |  [ami\\_housekeeper\\_lambda\\_schedule\\_expression](#input\\_ami\\_housekeeper\\_lambda\\_schedule\\_expression) | Scheduler expression for action runner binary syncer. | `string` | `\"rate(1 day)\"` | no | |  [ami\\_housekeeper\\_lambda\\_timeout](#input\\_ami\\_housekeeper\\_lambda\\_timeout) | Time out of the lambda in seconds. | `number` | `300` | no | |  [ami\\_housekeeper\\_lambda\\_zip](#input\\_ami\\_housekeeper\\_lambda\\_zip) | File location of the lambda zip file. | `string` | `null` | no | |  [ami\\_id\\_ssm\\_parameter\\_name](#input\\_ami\\_id\\_ssm\\_parameter\\_name) | Externally managed SSM parameter (of data type aws:ec2:image) that contains the AMI ID to launch runner instances from. Overrides ami\\_filter | `string` | `null` | no | |  [ami\\_kms\\_key\\_arn](#input\\_ami\\_kms\\_key\\_arn) | Optional CMK Key ARN to be used to launch an instance from a shared encrypted AMI | `string` | `null` | no | |  [ami\\_owners](#input\\_ami\\_owners) | The list of owners used to select the AMI of action runner instances. | `list(string)` | <pre>[  \"amazon\"]</pre> | no | |  [associate\\_public\\_ipv4\\_address](#input\\_associate\\_public\\_ipv4\\_address) | Associate public IPv4 with the runner. Only tested with IPv4 | `bool` | `false` | no | |  [aws\\_partition](#input\\_aws\\_partition) | (optiona) partition in the arn namespace to use if not 'aws' | `string` | `\"aws\"` | no | |  [aws\\_region](#input\\_aws\\_region) | AWS region. | `string` | n/a | yes | |  [block\\_device\\_mappings](#input\\_block\\_device\\_mappings) | The EC2 instance block device configuration. Takes the following keys: `device_name`, `delete_on_termination`, `volume_type`, `volume_size`, `encrypted`, `iops`, `throughput`, `kms_key_id`, `snapshot_id`. | <pre>list(object({    delete_on_termination = optional(bool, true)    device_name           = optional(string, \"/dev/xvda\")    encrypted             = optional(bool, true)    iops                  = optional(number)    kms_key_id            = optional(string)    snapshot_id           = optional(string)    throughput            = optional(number)    volume_size           = number    volume_type           = optional(string, \"gp3\")  }))</pre> | <pre>[  {    \"volume_size\": 30  }]</pre> | no | |  [cloudwatch\\_config](#input\\_cloudwatch\\_config) | (optional) Replaces the module's default cloudwatch log config. See https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Agent-Configuration-File-Details.html for details. | `string` | `null` | no | |  [create\\_service\\_linked\\_role\\_spot](#input\\_create\\_service\\_linked\\_role\\_spot) | (optional) create the service linked role for spot instances that is required by the scale-up lambda. | `bool` | `false` | no | |  [delay\\_webhook\\_event](#input\\_delay\\_webhook\\_event) | The number of seconds the event accepted by the webhook is invisible on the queue before the scale up lambda will receive the event. | `number` | `30` | no | |  [disable\\_runner\\_autoupdate](#input\\_disable\\_runner\\_autoupdate) | Disable the auto update of the github runner agent. Be aware there is a grace period of 30 days, see also the [GitHub article](https://github.blog/changelog/2022-02-01-github-actions-self-hosted-runners-can-now-disable-automatic-updates/) | `bool` | `false` | no | |  [enable\\_ami\\_housekeeper](#input\\_enable\\_ami\\_housekeeper) | Option to disable the lambda to clean up old AMIs. | `bool` | `false` | no | |  [enable\\_cloudwatch\\_agent](#input\\_enable\\_cloudwatch\\_agent) | Enables the cloudwatch agent on the ec2 runner instances. The runner uses a default config that can be overridden via `cloudwatch_config`. | `bool` | `true` | no | |  [enable\\_ephemeral\\_runners](#input\\_enable\\_ephemeral\\_runners) | Enable ephemeral runners, runners will only be used once. | `bool` | `false` | no | |  [enable\\_event\\_rule\\_binaries\\_syncer](#input\\_enable\\_event\\_rule\\_binaries\\_syncer) | Option to disable EventBridge Lambda trigger for the binary syncer, useful to stop automatic updates of binary distribution. | `bool` | `true` | no | |  [enable\\_fifo\\_build\\_queue](#input\\_enable\\_fifo\\_build\\_queue) | Enable a FIFO queue to keep the order of events received by the webhook. Recommended for repo level runners. | `bool` | `false` | no | |  [enable\\_jit\\_config](#input\\_enable\\_jit\\_config) | Overwrite the default behavior for JIT configuration. By default JIT configuration is enabled for ephemeral runners and disabled for non-ephemeral runners. In case of GHES check first if the JIT config API is avaialbe. In case you upgradeing from 3.x to 4.x you can set `enable_jit_config` to `false` to avoid a breaking change when having your own AMI. | `bool` | `null` | no | |  [enable\\_job\\_queued\\_check](#input\\_enable\\_job\\_queued\\_check) | Only scale if the job event received by the scale up lambda is in the queued state. By default enabled for non ephemeral runners and disabled for ephemeral. Set this variable to overwrite the default behavior. | `bool` | `null` | no | |  [enable\\_managed\\_runner\\_security\\_group](#input\\_enable\\_managed\\_runner\\_security\\_group) | Enables creation of the default managed security group. Unmanaged security groups can be specified via `runner_additional_security_group_ids`. | `bool` | `true` | no | |  [enable\\_organization\\_runners](#input\\_enable\\_organization\\_runners) | Register runners to organization, instead of repo level | `bool` | `false` | no | |  [enable\\_runner\\_binaries\\_syncer](#input\\_enable\\_runner\\_binaries\\_syncer) | Option to disable the lambda to sync GitHub runner distribution, useful when using a pre-build AMI. | `bool` | `true` | no | |  [enable\\_runner\\_detailed\\_monitoring](#input\\_enable\\_runner\\_detailed\\_monitoring) | Should detailed monitoring be enabled for the runner. Set this to true if you want to use detailed monitoring. See https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-cloudwatch-new.html for details. | `bool` | `false` | no | |  [enable\\_runner\\_on\\_demand\\_failover\\_for\\_errors](#input\\_enable\\_runner\\_on\\_demand\\_failover\\_for\\_errors) | Enable on-demand failover. For example to fall back to on demand when no spot capacity is available the variable can be set to `InsufficientInstanceCapacity`. When not defined the default behavior is to retry later. | `list(string)` | `[]` | no | |  [enable\\_runner\\_workflow\\_job\\_labels\\_check\\_all](#input\\_enable\\_runner\\_workflow\\_job\\_labels\\_check\\_all) | If set to true all labels in the workflow job must match the GitHub labels (os, architecture and `self-hosted`). When false if __any__ label matches it will trigger the webhook. | `bool` | `true` | no | |  [enable\\_ssm\\_on\\_runners](#input\\_enable\\_ssm\\_on\\_runners) | Enable to allow access to the runner instances for debugging purposes via SSM. Note that this adds additional permissions to the runner instances. | `bool` | `false` | no | |  [enable\\_user\\_data\\_debug\\_logging\\_runner](#input\\_enable\\_user\\_data\\_debug\\_logging\\_runner) | Option to enable debug logging for user-data, this logs all secrets as well. | `bool` | `false` | no | |  [enable\\_userdata](#input\\_enable\\_userdata) | Should the userdata script be enabled for the runner. Set this to false if you are using your own prebuilt AMI. | `bool` | `true` | no | |  [enable\\_workflow\\_job\\_events\\_queue](#input\\_enable\\_workflow\\_job\\_events\\_queue) | Enabling this experimental feature will create a secondory sqs queue to which a copy of the workflow\\_job event will be delivered. | `bool` | `false` | no | |  [ghes\\_ssl\\_verify](#input\\_ghes\\_ssl\\_verify) | GitHub Enterprise SSL verification. Set to 'false' when custom certificate (chains) is used for GitHub Enterprise Server (insecure). | `bool` | `true` | no | |  [ghes\\_url](#input\\_ghes\\_url) | GitHub Enterprise Server URL. Example: https://github.internal.co - DO NOT SET IF USING PUBLIC GITHUB | `string` | `null` | no | |  [github\\_app](#input\\_github\\_app) | GitHub app parameters, see your github app. Ensure the key is the base64-encoded `.pem` file (the output of `base64 app.private-key.pem`, not the content of `private-key.pem`). | <pre>object({    key_base64     = string    id             = string    webhook_secret = string  })</pre> | n/a | yes | |  [idle\\_config](#input\\_idle\\_config) | List of time periods, defined as a cron expression, to keep a minimum amount of runners active instead of scaling down to 0. By defining this list you can ensure that in time periods that match the cron expression within 5 seconds a runner is kept idle. | <pre>list(object({    cron             = string    timeZone         = string    idleCount        = number    evictionStrategy = optional(string, \"oldest_first\")  }))</pre> | `[]` | no | |  [instance\\_allocation\\_strategy](#input\\_instance\\_allocation\\_strategy) | The allocation strategy for spot instances. AWS recommends using `price-capacity-optimized` however the AWS default is `lowest-price`. | `string` | `\"lowest-price\"` | no | |  [instance\\_max\\_spot\\_price](#input\\_instance\\_max\\_spot\\_price) | Max price price for spot instances per hour. This variable will be passed to the create fleet as max spot price for the fleet. | `string` | `null` | no | |  [instance\\_profile\\_path](#input\\_instance\\_profile\\_path) | The path that will be added to the instance\\_profile, if not set the environment name will be used. | `string` | `null` | no | |  [instance\\_target\\_capacity\\_type](#input\\_instance\\_target\\_capacity\\_type) | Default lifecycle used for runner instances, can be either `spot` or `on-demand`. | `string` | `\"spot\"` | no | |  [instance\\_types](#input\\_instance\\_types) | List of instance types for the action runner. Defaults are based on runner\\_os (al2023 for linux and Windows Server Core for win). | `list(string)` | <pre>[  \"m5.large\",  \"c5.large\"]</pre> | no | |  [job\\_queue\\_retention\\_in\\_seconds](#input\\_job\\_queue\\_retention\\_in\\_seconds) | The number of seconds the job is held in the queue before it is purged. | `number` | `86400` | no | |  [key\\_name](#input\\_key\\_name) | Key pair name | `string` | `null` | no | |  [kms\\_key\\_arn](#input\\_kms\\_key\\_arn) | Optional CMK Key ARN to be used for Parameter Store. This key must be in the current account. | `string` | `null` | no | |  [lambda\\_architecture](#input\\_lambda\\_architecture) | AWS Lambda architecture. Lambda functions using Graviton processors ('arm64') tend to have better price/performance than 'x86\\_64' functions. | `string` | `\"arm64\"` | no | |  [lambda\\_principals](#input\\_lambda\\_principals) | (Optional) add extra principals to the role created for execution of the lambda, e.g. for local testing. | <pre>list(object({    type        = string    identifiers = list(string)  }))</pre> | `[]` | no | |  [lambda\\_runtime](#input\\_lambda\\_runtime) | AWS Lambda runtime. | `string` | `\"nodejs18.x\"` | no | |  [lambda\\_s3\\_bucket](#input\\_lambda\\_s3\\_bucket) | S3 bucket from which to specify lambda functions. This is an alternative to providing local files directly. | `string` | `null` | no | |  [lambda\\_security\\_group\\_ids](#input\\_lambda\\_security\\_group\\_ids) | List of security group IDs associated with the Lambda function. | `list(string)` | `[]` | no | |  [lambda\\_subnet\\_ids](#input\\_lambda\\_subnet\\_ids) | List of subnets in which the action runners will be launched, the subnets needs to be subnets in the `vpc_id`. | `list(string)` | `[]` | no | |  [lambda\\_tracing\\_mode](#input\\_lambda\\_tracing\\_mode) | DEPRECATED: Replaced by `tracing_config`. | `string` | `null` | no | |  [log\\_level](#input\\_log\\_level) | Logging level for lambda logging. Valid values are  'silly', 'trace', 'debug', 'info', 'warn', 'error', 'fatal'. | `string` | `\"info\"` | no | |  [logging\\_kms\\_key\\_id](#input\\_logging\\_kms\\_key\\_id) | Specifies the kms key id to encrypt the logs with. | `string` | `null` | no | |  [logging\\_retention\\_in\\_days](#input\\_logging\\_retention\\_in\\_days) | Specifies the number of days you want to retain log events for the lambda log group. Possible values are: 0, 1, 3, 5, 7, 14, 30, 60, 90, 120, 150, 180, 365, 400, 545, 731, 1827, and 3653. | `number` | `180` | no | |  [minimum\\_running\\_time\\_in\\_minutes](#input\\_minimum\\_running\\_time\\_in\\_minutes) | The time an ec2 action runner should be running at minimum before terminated, if not busy. | `number` | `null` | no | |  [pool\\_config](#input\\_pool\\_config) | The configuration for updating the pool. The `pool_size` to adjust to by the events triggered by the `schedule_expression`. For example you can configure a cron expression for weekdays to adjust the pool to 10 and another expression for the weekend to adjust the pool to 1. | <pre>list(object({    schedule_expression = string    size                = number  }))</pre> | `[]` | no | |  [pool\\_lambda\\_reserved\\_concurrent\\_executions](#input\\_pool\\_lambda\\_reserved\\_concurrent\\_executions) | Amount of reserved concurrent executions for the scale-up lambda function. A value of 0 disables lambda from being triggered and -1 removes any concurrency limitations. | `number` | `1` | no | |  [pool\\_lambda\\_timeout](#input\\_pool\\_lambda\\_timeout) | Time out for the pool lambda in seconds. | `number` | `60` | no | |  [pool\\_runner\\_owner](#input\\_pool\\_runner\\_owner) | The pool will deploy runners to the GitHub org ID, set this value to the org to which you want the runners deployed. Repo level is not supported. | `string` | `null` | no | |  [prefix](#input\\_prefix) | The prefix used for naming resources | `string` | `\"github-actions\"` | no | |  [queue\\_encryption](#input\\_queue\\_encryption) | Configure how data on queues managed by the modules in ecrypted at REST. Options are encryped via SSE, non encrypted and via KMSS. By default encryptes via SSE is enabled. See for more details the Terraform `aws_sqs_queue` resource https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/sqs_queue. | <pre>object({    kms_data_key_reuse_period_seconds = number    kms_master_key_id                 = string    sqs_managed_sse_enabled           = bool  })</pre> | <pre>{  \"kms_data_key_reuse_period_seconds\": null,  \"kms_master_key_id\": null,  \"sqs_managed_sse_enabled\": true}</pre> | no | |  [redrive\\_build\\_queue](#input\\_redrive\\_build\\_queue) | Set options to attach (optional) a dead letter queue to the build queue, the queue between the webhook and the scale up lambda. You have the following options. 1. Disable by setting `enabled` to false. 2. Enable by setting `enabled` to `true`, `maxReceiveCount` to a number of max retries. | <pre>object({    enabled         = bool    maxReceiveCount = number  })</pre> | <pre>{  \"enabled\": false,  \"maxReceiveCount\": null}</pre> | no | |  [repository\\_white\\_list](#input\\_repository\\_white\\_list) | List of github repository full names (owner/repo\\_name) that will be allowed to use the github app. Leave empty for no filtering. | `list(string)` | `[]` | no | |  [role\\_path](#input\\_role\\_path) | The path that will be added to role path for created roles, if not set the environment name will be used. | `string` | `null` | no | |  [role\\_permissions\\_boundary](#input\\_role\\_permissions\\_boundary) | Permissions boundary that will be added to the created roles. | `string` | `null` | no | |  [runner\\_additional\\_security\\_group\\_ids](#input\\_runner\\_additional\\_security\\_group\\_ids) | (optional) List of additional security groups IDs to apply to the runner. | `list(string)` | `[]` | no | |  [runner\\_architecture](#input\\_runner\\_architecture) | The platform architecture of the runner instance\\_type. | `string` | `\"x64\"` | no | |  [runner\\_as\\_root](#input\\_runner\\_as\\_root) | Run the action runner under the root user. Variable `runner_run_as` will be ignored. | `bool` | `false` | no | |  [runner\\_binaries\\_s3\\_logging\\_bucket](#input\\_runner\\_binaries\\_s3\\_logging\\_bucket) | Bucket for action runner distribution bucket access logging. | `string` | `null` | no | |  [runner\\_binaries\\_s3\\_logging\\_bucket\\_prefix](#input\\_runner\\_binaries\\_s3\\_logging\\_bucket\\_prefix) | Bucket prefix for action runner distribution bucket access logging. | `string` | `null` | no | |  [runner\\_binaries\\_s3\\_sse\\_configuration](#input\\_runner\\_binaries\\_s3\\_sse\\_configuration) | Map containing server-side encryption configuration for runner-binaries S3 bucket. | `any` | <pre>{  \"rule\": {    \"apply_server_side_encryption_by_default\": {      \"sse_algorithm\": \"AES256\"    }  }}</pre> | no | |  [runner\\_binaries\\_s3\\_versioning](#input\\_runner\\_binaries\\_s3\\_versioning) | Status of S3 versioning for runner-binaries S3 bucket. Once set to Enabled the change cannot be reverted via Terraform! | `string` | `\"Disabled\"` | no | |  [runner\\_binaries\\_syncer\\_lambda\\_timeout](#input\\_runner\\_binaries\\_syncer\\_lambda\\_timeout) | Time out of the binaries sync lambda in seconds. | `number` | `300` | no | |  [runner\\_binaries\\_syncer\\_lambda\\_zip](#input\\_runner\\_binaries\\_syncer\\_lambda\\_zip) | File location of the binaries sync lambda zip file. | `string` | `null` | no | |  [runner\\_boot\\_time\\_in\\_minutes](#input\\_runner\\_boot\\_time\\_in\\_minutes) | The minimum time for an EC2 runner to boot and register as a runner. | `number` | `5` | no | |  [runner\\_credit\\_specification](#input\\_runner\\_credit\\_specification) | The credit option for CPU usage of a T instance. Can be unset, \"standard\" or \"unlimited\". | `string` | `null` | no | |  [runner\\_ec2\\_tags](#input\\_runner\\_ec2\\_tags) | Map of tags that will be added to the launch template instance tag specifications. | `map(string)` | `{}` | no | |  [runner\\_egress\\_rules](#input\\_runner\\_egress\\_rules) | List of egress rules for the GitHub runner instances. | <pre>list(object({    cidr_blocks      = list(string)    ipv6_cidr_blocks = list(string)    prefix_list_ids  = list(string)    from_port        = number    protocol         = string    security_groups  = list(string)    self             = bool    to_port          = number    description      = string  }))</pre> | <pre>[  {    \"cidr_blocks\": [      \"0.0.0.0/0\"    ],    \"description\": null,    \"from_port\": 0,    \"ipv6_cidr_blocks\": [      \"::/0\"    ],    \"prefix_list_ids\": null,    \"protocol\": \"-1\",    \"security_groups\": null,    \"self\": null,    \"to_port\": 0  }]</pre> | no | |  [runner\\_extra\\_labels](#input\\_runner\\_extra\\_labels) | Extra (custom) labels for the runners (GitHub). Labels checks on the webhook can be enforced by setting `enable_workflow_job_labels_check`. GitHub read-only labels should not be provided. | `list(string)` | `[]` | no | |  [runner\\_group\\_name](#input\\_runner\\_group\\_name) | Name of the runner group. | `string` | `\"Default\"` | no | |  [runner\\_iam\\_role\\_managed\\_policy\\_arns](#input\\_runner\\_iam\\_role\\_managed\\_policy\\_arns) | Attach AWS or customer-managed IAM policies (by ARN) to the runner IAM role | `list(string)` | `[]` | no | |  [runner\\_log\\_files](#input\\_runner\\_log\\_files) | (optional) Replaces the module default cloudwatch log config. See https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Agent-Configuration-File-Details.html for details. | <pre>list(object({    log_group_name   = string    prefix_log_group = bool    file_path        = string    log_stream_name  = string  }))</pre> | `null` | no | |  [runner\\_metadata\\_options](#input\\_runner\\_metadata\\_options) | Metadata options for the ec2 runner instances. By default, the module uses metadata tags for bootstrapping the runner, only disable `instance_metadata_tags` when using custom scripts for starting the runner. | `map(any)` | <pre>{  \"http_endpoint\": \"enabled\",  \"http_put_response_hop_limit\": 1,  \"http_tokens\": \"required\",  \"instance_metadata_tags\": \"enabled\"}</pre> | no | |  [runner\\_name\\_prefix](#input\\_runner\\_name\\_prefix) | The prefix used for the GitHub runner name. The prefix will be used in the default start script to prefix the instance name when register the runner in GitHub. The value is availabe via an EC2 tag 'ghr:runner\\_name\\_prefix'. | `string` | `\"\"` | no | |  [runner\\_os](#input\\_runner\\_os) | The EC2 Operating System type to use for action runner instances (linux,windows). | `string` | `\"linux\"` | no | |  [runner\\_run\\_as](#input\\_runner\\_run\\_as) | Run the GitHub actions agent as user. | `string` | `\"ec2-user\"` | no | |  [runners\\_lambda\\_s3\\_key](#input\\_runners\\_lambda\\_s3\\_key) | S3 key for runners lambda function. Required if using S3 bucket to specify lambdas. | `string` | `null` | no | |  [runners\\_lambda\\_s3\\_object\\_version](#input\\_runners\\_lambda\\_s3\\_object\\_version) | S3 object version for runners lambda function. Useful if S3 versioning is enabled on source bucket. | `string` | `null` | no | |  [runners\\_lambda\\_zip](#input\\_runners\\_lambda\\_zip) | File location of the lambda zip file for scaling runners. | `string` | `null` | no | |  [runners\\_maximum\\_count](#input\\_runners\\_maximum\\_count) | The maximum number of runners that will be created. | `number` | `3` | no | |  [runners\\_scale\\_down\\_lambda\\_timeout](#input\\_runners\\_scale\\_down\\_lambda\\_timeout) | Time out for the scale down lambda in seconds. | `number` | `60` | no | |  [runners\\_scale\\_up\\_lambda\\_timeout](#input\\_runners\\_scale\\_up\\_lambda\\_timeout) | Time out for the scale up lambda in seconds. | `number` | `30` | no | |  [runners\\_ssm\\_housekeeper](#input\\_runners\\_ssm\\_housekeeper) | Configuration for the SSM housekeeper lambda. This lambda deletes token / JIT config from SSM.  `schedule_expression`: is used to configure the schedule for the lambda.  `enabled`: enable or disable the lambda trigger via the EventBridge.  `lambda_timeout`: timeout for the lambda in seconds.  `config`: configuration for the lambda function. Token path will be read by default from the module. | <pre>object({    schedule_expression = optional(string, \"rate(1 day)\")    enabled             = optional(bool, true)    lambda_timeout      = optional(number, 60)    config = object({      tokenPath      = optional(string)      minimumDaysOld = optional(number, 1)      dryRun         = optional(bool, false)    })  })</pre> | <pre>{  \"config\": {}}</pre> | no | |  [scale\\_down\\_schedule\\_expression](#input\\_scale\\_down\\_schedule\\_expression) | Scheduler expression to check every x for scale down. | `string` | `\"cron(*/5 * * * ? *)\"` | no | |  [scale\\_up\\_reserved\\_concurrent\\_executions](#input\\_scale\\_up\\_reserved\\_concurrent\\_executions) | Amount of reserved concurrent executions for the scale-up lambda function. A value of 0 disables lambda from being triggered and -1 removes any concurrency limitations. | `number` | `1` | no | |  [ssm\\_paths](#input\\_ssm\\_paths) | The root path used in SSM to store configuration and secrets. | <pre>object({    root       = optional(string, \"github-action-runners\")    app        = optional(string, \"app\")    runners    = optional(string, \"runners\")    use_prefix = optional(bool, true)  })</pre> | `{}` | no | |  [subnet\\_ids](#input\\_subnet\\_ids) | List of subnets in which the action runner instances will be launched. The subnets need to exist in the configured VPC (`vpc_id`), and must reside in different availability zones (see https://github.com/philips-labs/terraform-aws-github-runner/issues/2904) | `list(string)` | n/a | yes | |  [syncer\\_lambda\\_s3\\_key](#input\\_syncer\\_lambda\\_s3\\_key) | S3 key for syncer lambda function. Required if using an S3 bucket to specify lambdas. | `string` | `null` | no | |  [syncer\\_lambda\\_s3\\_object\\_version](#input\\_syncer\\_lambda\\_s3\\_object\\_version) | S3 object version for syncer lambda function. Useful if S3 versioning is enabled on source bucket. | `string` | `null` | no | |  [tags](#input\\_tags) | Map of tags that will be added to created resources. By default resources will be tagged with name and environment. | `map(string)` | `{}` | no | |  [tracing\\_config](#input\\_tracing\\_config) | Configuration for lambda tracing. | <pre>object({    mode                  = optional(string, null)    capture_http_requests = optional(bool, false)    capture_error         = optional(bool, false)  })</pre> | `{}` | no | |  [userdata\\_post\\_install](#input\\_userdata\\_post\\_install) | Script to be ran after the GitHub Actions runner is installed on the EC2 instances | `string` | `\"\"` | no | |  [userdata\\_pre\\_install](#input\\_userdata\\_pre\\_install) | Script to be ran before the GitHub Actions runner is installed on the EC2 instances | `string` | `\"\"` | no | |  [userdata\\_template](#input\\_userdata\\_template) | Alternative user-data template, replacing the default template. By providing your own user\\_data you have to take care of installing all required software, including the action runner. Variables userdata\\_pre/post\\_install are ignored. | `string` | `null` | no | |  [vpc\\_id](#input\\_vpc\\_id) | The VPC for security groups of the action runners. | `string` | n/a | yes | |  [webhook\\_lambda\\_apigateway\\_access\\_log\\_settings](#input\\_webhook\\_lambda\\_apigateway\\_access\\_log\\_settings) | Access log settings for webhook API gateway. | <pre>object({    destination_arn = string    format          = string  })</pre> | `null` | no | |  [webhook\\_lambda\\_s3\\_key](#input\\_webhook\\_lambda\\_s3\\_key) | S3 key for webhook lambda function. Required if using S3 bucket to specify lambdas. | `string` | `null` | no | |  [webhook\\_lambda\\_s3\\_object\\_version](#input\\_webhook\\_lambda\\_s3\\_object\\_version) | S3 object version for webhook lambda function. Useful if S3 versioning is enabled on source bucket. | `string` | `null` | no | |  [webhook\\_lambda\\_timeout](#input\\_webhook\\_lambda\\_timeout) | Time out of the webhook lambda in seconds. | `number` | `10` | no | |  [webhook\\_lambda\\_zip](#input\\_webhook\\_lambda\\_zip) | File location of the webhook lambda zip file. | `string` | `null` | no | |  [workflow\\_job\\_queue\\_configuration](#input\\_workflow\\_job\\_queue\\_configuration) | Configuration options for workflow job queue which is only applicable if the flag enable\\_workflow\\_job\\_events\\_queue is set to true. | <pre>object({    delay_seconds              = number    visibility_timeout_seconds = number    message_retention_seconds  = number  })</pre> | <pre>{  \"delay_seconds\": null,  \"message_retention_seconds\": null,  \"visibility_timeout_seconds\": null}</pre> | no |  ## Outputs  | Name | Description | |------|-------------| |  [binaries\\_syncer](#output\\_binaries\\_syncer) | n/a | |  [queues](#output\\_queues) | SQS queues. | |  [runners](#output\\_runners) | n/a | |  [ssm\\_parameters](#output\\_ssm\\_parameters) | n/a | |  [webhook](#output\\_webhook) | n/a |"},{"location":"security.html#contributing","title":"Contributing","text":"<p>We welcome contributions, please check out the contribution guide. Be aware we use pre commit hooks to update the docs.</p>"},{"location":"security.html#philips-forest","title":"Philips Forest","text":"<p>This module is part of the Philips Forest.</p> <pre><code>                                                     ___                   _\n                                                    / __\\__  _ __ ___  ___| |_\n                                                   / _\\/ _ \\| '__/ _ \\/ __| __|\n                                                  / / | (_) | | |  __/\\__ \\ |_\n                                                  \\/   \\___/|_|  \\___||___/\\__|\n\n                                                                 Infrastructure\n</code></pre> <p>Talk to the forestkeepers in the <code>runners-channel</code> on Slack.</p> <p></p> <p>test</p>"},{"location":"setup.html","title":"Setup","text":"<p>Terraform examples are availabile for different use-cases for example multiple runners, ephemeral runners, and windows. For more details see the examplees.</p> <p>The module supports two main scenarios for creating runners. Repository level runners will be dedicated to only one repository, no other repository can use the runner. At the organization level you can use the runner(s) for all repositories within the organization. See GitHub self-hosted runner instructions for more information. Before starting the deployment you have to choose one option.</p> <p>The setup guide below is a generic direction. There are many choices you can make, and there is no right way. For example we deploy ephemeral runners for both Linux and WIndows with packer pre-bult AMI's that are automatically updated. Deployment is done with GitHub actions, Terragrunt and terraform. The lamba's we sync to AWS S3. For the major fleet we have a tiny pool to let start jobs quickly.</p>"},{"location":"setup.html#required-rools","title":"Required rools","text":"<p>The following tools are a minimum rqquirement. We advise to deploy the stack via a CI/CD pipeline.</p> <ul> <li>Terraform</li> <li>Bash shell or compatible</li> <li>Docker (optional, to build lambdas without node).</li> <li>AWS cli (optional)</li> <li>Node and yarn to build the Lambda's (or download via Release).</li> </ul>"},{"location":"setup.html#setup-guide","title":"Setup guide","text":"<p>The setup consists of running Terraform to create all AWS resources and manually configuring the GitHub App. The Terraform module requires configuration from the GitHub App and the GitHub app requires output from Terraform. Therefore you first create the GitHub App and configure the basics, then run Terraform, and afterwards finalize the configuration of the GitHub App.</p>"},{"location":"setup.html#setup-github-app-part-1","title":"Setup GitHub App (part 1)","text":"<p>Go to GitHub and create a new app. Be aware you can create apps for your organization or for a user. For now we only support organization level apps.</p> <ol> <li>Create an app in Github</li> <li>Choose a name</li> <li>Choose a website (mandatory, not required for the module).</li> <li>Disable the webhook for now (we will configure this later or create an alternative webhook).</li> <li>Permissions for all runners:<ul> <li>Repository:</li> <li><code>Actions</code>: Read-only (check for queued jobs)</li> <li><code>Checks</code>: Read-only (receive events for new builds)</li> <li><code>Metadata</code>: Read-only (default/required)</li> </ul> </li> <li>Permissions for repo level runners only:</li> <li>Repository:<ul> <li><code>Administration</code>: Read &amp; write (to register runner)</li> </ul> </li> <li>Permissions for organization level runners only:</li> <li>Organization<ul> <li><code>Self-hosted runners</code>: Read &amp; write (to register runner)</li> </ul> </li> <li>Save the new app.</li> <li>On the General page, make a note of the \"App ID\" and \"Client ID\" parameters.</li> <li>Generate a new private key and save the <code>app.private-key.pem</code> file.</li> </ol>"},{"location":"setup.html#setup-terraform-module","title":"Setup terraform module","text":""},{"location":"setup.html#download-lambdas","title":"Download lambdas","text":"<p>To apply the terraform module, the compiled lambdas (.zip files) need to be available either locally or in an S3 bucket. They can either be downloaded from the GitHub release page or built locally.</p> <p>To read the files from S3, set the <code>lambda_s3_bucket</code> variable and the specific object key for each lambda.</p> <p>The lambdas can be downloaded manually from the release page or using the download-lambda terraform module (requires <code>curl</code> to be installed on your machine). In the <code>download-lambda</code> directory, run <code>terraform init &amp;&amp; terraform apply</code>. The lambdas will be saved to the same directory.</p> <p>For local development you can build all the lambdas at once using <code>.ci/build.sh</code> or individually using <code>yarn dist</code>.</p>"},{"location":"setup.html#service-linked-role","title":"Service-linked role","text":"<p>To create spot instances the <code>AWSServiceRoleForEC2Spot</code> role needs to be added to your account. You can do that manually by following the AWS docs. To use terraform for creating the role, either add the following resource or let the module manage the service linked role by setting <code>create_service_linked_role_spot</code> to <code>true</code>. Be aware this is an account global role, so maybe you don't want to manage it via a specific deployment.</p> <pre><code>resource \"aws_iam_service_linked_role\" \"spot\" {\n  aws_service_name = \"spot.amazonaws.com\"\n}\n</code></pre>"},{"location":"setup.html#terraform-module","title":"Terraform module","text":"<p>Next create a second terraform workspace and initiate the module, or adapt one of the examples.</p> <p>Note that <code>github_app.key_base64</code> needs to be a base64-encoded string of the <code>.pem</code> file i.e. the output of <code>base64 app.private-key.pem</code>. The decoded string can either be a multiline value or a single line value with new lines represented with literal <code>\\n</code> characters.</p> <pre><code>module \"github-runner\" {\n  source  = \"philips-labs/github-runner/aws\"\n  version = \"REPLACE_WITH_VERSION\"\n\n  aws_region = \"eu-west-1\"\n  vpc_id     = \"vpc-123\"\n  subnet_ids = [\"subnet-123\", \"subnet-456\"]\n\n  prefix = \"gh-ci\"\n\n  github_app = {\n    key_base64     = \"base64string\"\n    id             = \"1\"\n    webhook_secret = \"webhook_secret\"\n  }\n\n  webhook_lambda_zip                = \"lambdas-download/webhook.zip\"\n  runner_binaries_syncer_lambda_zip = \"lambdas-download/runner-binaries-syncer.zip\"\n  runners_lambda_zip                = \"lambdas-download/runners.zip\"\n  enable_organization_runners = true\n}\n</code></pre> <p>Run terraform by using the following commands</p> <pre><code>terraform init\nterraform apply\n</code></pre> <p>The terraform output displays the API gateway url (endpoint) and secret, which you need in the next step.</p> <p>The lambda for syncing the GitHub distribution to S3 is triggered via CloudWatch (by default once per hour). After deployment the function is triggered via S3 to ensure the distribution is cached.</p>"},{"location":"setup.html#setup-the-webhook-github-app-part-2","title":"Setup the webhook / GitHub App (part 2)","text":"<p>At this point you have two options. Either create a separate webhook (enterprise, org, or repo), or create a webhook in the App.</p>"},{"location":"setup.html#option-1-webhook","title":"Option 1: Webhook","text":"<ol> <li>Create a new webhook at the repo level for repo level runners, or org (or enterprise level) for org level runners.</li> <li>Provide the webhook url, which should be part of the output of terraform.</li> <li>Provide the webhook secret (<code>terraform output -raw &lt;NAME_OUTPUT_VAR&gt;</code>).</li> <li>Ensure the content type is <code>application/json</code>.</li> <li>In the \"Permissions &amp; Events\" section and then \"Subscribe to Events\" subsection, check either \"Workflow Job\" or \"Check Run\" (choose only one option!!!).</li> <li>In the \"Install App\" section, install the App in your organization, either in all or in selected repositories.</li> </ol>"},{"location":"setup.html#option-2-app","title":"Option 2: App","text":"<p>Go back to the GitHub App and update the following settings.</p> <ol> <li>Enable the webhook.</li> <li>Provide the webhook url, should be part of the output of terraform.</li> <li>Provide the webhook secret (<code>terraform output -raw &lt;NAME_OUTPUT_VAR&gt;</code>).</li> <li>In the \"Permissions &amp; Events\" section and then \"Subscribe to Events\" subsection, check either \"Workflow Job\" or \"Check Run\" (choose only one option!!!).</li> </ol>"},{"location":"setup.html#install-github-app","title":"Install GitHub app","text":"<p>Finally you need to ensure the app is installed to all or selected repositories.</p> <p>Go back to the GitHub App and update the following settings.</p> <ol> <li>In the \"Install App\" section, install the App in your organization, either in all or in selected repositories.</li> </ol>"},{"location":"setup.html#debugging","title":"Debugging","text":"<p>In case the setup does not work as intended follow the trace of events:</p> <ul> <li>In the GitHub App configuration, the Advanced page displays all webhook events that were sent.</li> <li>In AWS CloudWatch, every lambda has a log group. Look at the logs of the <code>webhook</code> and <code>scale-up</code> lambdas.</li> <li>In AWS SQS you can see messages available or in flight.</li> <li>Once an EC2 instance is running, you can connect to it in the EC2 user interface using Session Manager (use <code>enable_ssm_on_runners = true</code>). Check the user data script using <code>cat /var/log/user-data.log</code>. By default several log files of the instances are streamed to AWS CloudWatch, look for a log group named <code>&lt;environment&gt;/runners</code>. In the log group you should see at least the log streams for the user data installation and runner agent.</li> <li>Registered instances should show up in the Settings - Actions page of the repository or organization (depending on the installation mode).</li> </ul>"},{"location":"test-lambda-local.html","title":"Lambda - Test locally","text":"<p>This README provides guidance for testing the lambda locally / and or in AWS. This guide assumes you are familiar with AWS, lambda and Node. If not mentioned explicitly, comments provided should be executed from the root of the lambda package.</p>"},{"location":"test-lambda-local.html#testing-in-aws","title":"Testing in AWS","text":"<p>Just navigate to the Lambda in the AWS Console and trigger a test event. Provide an event that matches the required input. For lambdas that does not require a specific event, just send any event.</p>"},{"location":"test-lambda-local.html#testing-locally","title":"Testing locally","text":"<p>Testing locally can be done in two ways; using AWS SAM framework or run via a wrapper to simulate the event to invoke the lambda. Both setups require that the mandatory input environment variables be set, and AWS resources on which the lambda depends are available. We advise for testing the lambda locally to first create your own deployment of the whole module to AWS, this will simplify the setup of dependent AWS resources. For example, based on the de default example.</p> <p>Local test setup instructions are available for the following lambda's:</p> <ul> <li>runner-binary-syncer - This lambda does not need any input, no event is required. Supported via SAM and local Node.</li> </ul>"},{"location":"test-lambda-local.html#extend-deployment-configuration","title":"Extend deployment configuration","text":"<p>Add the code below to your Terraform deployment to allow your principal to use the Lambda role and retrieve the lambda configuration. Update your Terraform deployment and apply the changes.</p> <pre><code>data \"aws_caller_identity\" \"current\" {}\n\nmodule \"runners\" {\n\n  ...\n\n  # Assume you have a profile with Admin privileges, allow you to switch to the Lambda role\n  lambda_principals = [{\n    type        = \"AWS\"\n    identifiers = [\"arn:aws:iam::${data.aws_caller_identity.current.account_id}:root\"]\n  }]\n\n}\n\noutput \"development\" {\n  value = {\n    lambda_syncer = module.runners.binaries_syncer.lambda\n  }\n}\n</code></pre> <p>Once you have updated your Terraform deployment you need to read the lambda configuration into your environment. Run the commands below in your Terraform workspace folder.</p> <pre><code>LAMBDA_ENV=$(terraform output -json development | jq -r '.lambda_syncer.environment[].variables' | jq -r \"to_entries|map(\\\"\\(.key)=\\(.value|tostring)\\\")|.[]\")\nfor x in $LAMBDA_ENV ; do echo setting $x; export $x; done\n</code></pre>"},{"location":"test-lambda-local.html#testing-with-sam","title":"Testing with SAM","text":"<p>This setup requires AWS SAM CLI and Docker is installed locally. First update the AWS config (<code>~/.aws/config</code>) so you can use easily switch to the role used by the lambda.</p> <pre><code>[profile gh-development]\nsource_profile=&lt;OPTIONAL_SOURCE_PROFILE&gt;\nregion=&lt;DEFAULT_REGION&gt;\nrole_arn=&lt;ARN_CHECK_TF_OUTPUT&gt;\n</code></pre> <p>Now you can set the profile and region as environment variables or pass as argument to SAM.</p> <pre><code>export AWS_REGION=&lt;region&gt;\nexport AWS_PROFILE=gh-development\n</code></pre> <p>For SAM a <code>template.yml</code> defines the lambda for running locally. Thats all, now build your lambda with <code>yarn run dist</code> and then invoke the lambda with <code>sam local invoke</code>.</p>"},{"location":"test-lambda-local.html#with-node","title":"With Node","text":"<p>Instead of using SAM you can use Node with <code>ts-node-dev</code> to test the code locally. The drawback is that you have to setup AWS credentials in your shell. Also, you are dependent on a tiny wrapper (<code>local.ts</code>), and your local Node version.</p> <p>The AWS SDK does not seem to handle environment variables for profiles, the only option to pass the role is via credentials. You can get credentials via STS for the role.</p> <pre><code>role=$(aws sts assume-role \\\n    --role-arn \"&lt;ROLE&gt;\" \\\n    --duration-seconds 3600 --role-session-name \"dev\")\n\nexport AWS_ACCESS_KEY_ID=$(echo $role | jq -r .Credentials.AccessKeyId)\nexport AWS_SECRET_ACCESS_KEY=$(echo $role | jq -r .Credentials.SecretAccessKey)\nexport AWS_SESSION_TOKEN=$(echo $role | jq -r .Credentials.SessionToken)\n</code></pre> <p>Next set the region <code>export AWS_REGION=&lt;region&gt;</code>. Now you can run the lambda locally via <code>yarn run watch</code>.</p>"},{"location":"modules/runners.html","title":"Runners (root)","text":""},{"location":"modules/runners.html#requirements","title":"Requirements","text":"Name Version terraform &gt;= 1.3.0 aws ~&gt; 5.2 random ~&gt; 3.0"},{"location":"modules/runners.html#providers","title":"Providers","text":"Name Version aws ~&gt; 5.2 random ~&gt; 3.0"},{"location":"modules/runners.html#modules","title":"Modules","text":"Name Source Version ami_housekeeper ./modules/ami-housekeeper n/a runner_binaries ./modules/runner-binaries-syncer n/a runners ./modules/runners n/a ssm ./modules/ssm n/a webhook ./modules/webhook n/a"},{"location":"modules/runners.html#resources","title":"Resources","text":"Name Type aws_sqs_queue.queued_builds resource aws_sqs_queue.queued_builds_dlq resource aws_sqs_queue.webhook_events_workflow_job_queue resource aws_sqs_queue_policy.build_queue_dlq_policy resource aws_sqs_queue_policy.build_queue_policy resource aws_sqs_queue_policy.webhook_events_workflow_job_queue_policy resource random_string.random resource aws_iam_policy_document.deny_unsecure_transport data source"},{"location":"modules/runners.html#inputs","title":"Inputs","text":"Name Description Type Default Required ami_filter Map of lists used to create the AMI filter for the action runner AMI. <code>map(list(string))</code> <pre>{  \"state\": [    \"available\"  ]}</pre> no ami_housekeeper_cleanup_config Configuration for AMI cleanup. <code>amiFilters</code> - Filters to use when searching for AMIs to cleanup. Default filter for images owned by the account and that are available. <code>dryRun</code> - If true, no AMIs will be deregistered. Default false. <code>launchTemplateNames</code> - Launch template names to use when searching for AMIs to cleanup. Default no launch templates. <code>maxItems</code> - The maximum numer of AMI's tha will be queried for cleanup. Default no maximum. <code>minimumDaysOld</code> - Minimum number of days old an AMI must be to be considered for cleanup. Default 30. <code>ssmParameterNames</code> - SSM parameter names to use when searching for AMIs to cleanup. This parameter should be set when using SSM to configure the AMI to use. Default no SSM parameters. <pre>object({    amiFilters = optional(list(object({      Name   = string      Values = list(string)      })),      [{        Name : \"state\",        Values : [\"available\"],        },        {          Name : \"image-type\",          Values : [\"machine\"],      }]    )    dryRun              = optional(bool, false)    launchTemplateNames = optional(list(string))    maxItems            = optional(number)    minimumDaysOld      = optional(number, 30)    ssmParameterNames   = optional(list(string))  })</pre> <code>{}</code> no ami_housekeeper_lambda_s3_key S3 key for syncer lambda function. Required if using S3 bucket to specify lambdas. <code>string</code> <code>null</code> no ami_housekeeper_lambda_s3_object_version S3 object version for syncer lambda function. Useful if S3 versioning is enabled on source bucket. <code>string</code> <code>null</code> no ami_housekeeper_lambda_schedule_expression Scheduler expression for action runner binary syncer. <code>string</code> <code>\"rate(1 day)\"</code> no ami_housekeeper_lambda_timeout Time out of the lambda in seconds. <code>number</code> <code>300</code> no ami_housekeeper_lambda_zip File location of the lambda zip file. <code>string</code> <code>null</code> no ami_id_ssm_parameter_name Externally managed SSM parameter (of data type aws:ec2:image) that contains the AMI ID to launch runner instances from. Overrides ami_filter <code>string</code> <code>null</code> no ami_kms_key_arn Optional CMK Key ARN to be used to launch an instance from a shared encrypted AMI <code>string</code> <code>null</code> no ami_owners The list of owners used to select the AMI of action runner instances. <code>list(string)</code> <pre>[  \"amazon\"]</pre> no associate_public_ipv4_address Associate public IPv4 with the runner. Only tested with IPv4 <code>bool</code> <code>false</code> no aws_partition (optiona) partition in the arn namespace to use if not 'aws' <code>string</code> <code>\"aws\"</code> no aws_region AWS region. <code>string</code> n/a yes block_device_mappings The EC2 instance block device configuration. Takes the following keys: <code>device_name</code>, <code>delete_on_termination</code>, <code>volume_type</code>, <code>volume_size</code>, <code>encrypted</code>, <code>iops</code>, <code>throughput</code>, <code>kms_key_id</code>, <code>snapshot_id</code>. <pre>list(object({    delete_on_termination = optional(bool, true)    device_name           = optional(string, \"/dev/xvda\")    encrypted             = optional(bool, true)    iops                  = optional(number)    kms_key_id            = optional(string)    snapshot_id           = optional(string)    throughput            = optional(number)    volume_size           = number    volume_type           = optional(string, \"gp3\")  }))</pre> <pre>[  {    \"volume_size\": 30  }]</pre> no cloudwatch_config (optional) Replaces the module's default cloudwatch log config. See https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Agent-Configuration-File-Details.html for details. <code>string</code> <code>null</code> no create_service_linked_role_spot (optional) create the service linked role for spot instances that is required by the scale-up lambda. <code>bool</code> <code>false</code> no delay_webhook_event The number of seconds the event accepted by the webhook is invisible on the queue before the scale up lambda will receive the event. <code>number</code> <code>30</code> no disable_runner_autoupdate Disable the auto update of the github runner agent. Be aware there is a grace period of 30 days, see also the GitHub article <code>bool</code> <code>false</code> no enable_ami_housekeeper Option to disable the lambda to clean up old AMIs. <code>bool</code> <code>false</code> no enable_cloudwatch_agent Enables the cloudwatch agent on the ec2 runner instances. The runner uses a default config that can be overridden via <code>cloudwatch_config</code>. <code>bool</code> <code>true</code> no enable_ephemeral_runners Enable ephemeral runners, runners will only be used once. <code>bool</code> <code>false</code> no enable_event_rule_binaries_syncer Option to disable EventBridge Lambda trigger for the binary syncer, useful to stop automatic updates of binary distribution. <code>bool</code> <code>true</code> no enable_fifo_build_queue Enable a FIFO queue to keep the order of events received by the webhook. Recommended for repo level runners. <code>bool</code> <code>false</code> no enable_jit_config Overwrite the default behavior for JIT configuration. By default JIT configuration is enabled for ephemeral runners and disabled for non-ephemeral runners. In case of GHES check first if the JIT config API is avaialbe. In case you upgradeing from 3.x to 4.x you can set <code>enable_jit_config</code> to <code>false</code> to avoid a breaking change when having your own AMI. <code>bool</code> <code>null</code> no enable_job_queued_check Only scale if the job event received by the scale up lambda is in the queued state. By default enabled for non ephemeral runners and disabled for ephemeral. Set this variable to overwrite the default behavior. <code>bool</code> <code>null</code> no enable_managed_runner_security_group Enables creation of the default managed security group. Unmanaged security groups can be specified via <code>runner_additional_security_group_ids</code>. <code>bool</code> <code>true</code> no enable_organization_runners Register runners to organization, instead of repo level <code>bool</code> <code>false</code> no enable_runner_binaries_syncer Option to disable the lambda to sync GitHub runner distribution, useful when using a pre-build AMI. <code>bool</code> <code>true</code> no enable_runner_detailed_monitoring Should detailed monitoring be enabled for the runner. Set this to true if you want to use detailed monitoring. See https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-cloudwatch-new.html for details. <code>bool</code> <code>false</code> no enable_runner_on_demand_failover_for_errors Enable on-demand failover. For example to fall back to on demand when no spot capacity is available the variable can be set to <code>InsufficientInstanceCapacity</code>. When not defined the default behavior is to retry later. <code>list(string)</code> <code>[]</code> no enable_runner_workflow_job_labels_check_all If set to true all labels in the workflow job must match the GitHub labels (os, architecture and <code>self-hosted</code>). When false if any label matches it will trigger the webhook. <code>bool</code> <code>true</code> no enable_ssm_on_runners Enable to allow access to the runner instances for debugging purposes via SSM. Note that this adds additional permissions to the runner instances. <code>bool</code> <code>false</code> no enable_user_data_debug_logging_runner Option to enable debug logging for user-data, this logs all secrets as well. <code>bool</code> <code>false</code> no enable_userdata Should the userdata script be enabled for the runner. Set this to false if you are using your own prebuilt AMI. <code>bool</code> <code>true</code> no enable_workflow_job_events_queue Enabling this experimental feature will create a secondory sqs queue to which a copy of the workflow_job event will be delivered. <code>bool</code> <code>false</code> no ghes_ssl_verify GitHub Enterprise SSL verification. Set to 'false' when custom certificate (chains) is used for GitHub Enterprise Server (insecure). <code>bool</code> <code>true</code> no ghes_url GitHub Enterprise Server URL. Example: https://github.internal.co - DO NOT SET IF USING PUBLIC GITHUB <code>string</code> <code>null</code> no github_app GitHub app parameters, see your github app. Ensure the key is the base64-encoded <code>.pem</code> file (the output of <code>base64 app.private-key.pem</code>, not the content of <code>private-key.pem</code>). <pre>object({    key_base64     = string    id             = string    webhook_secret = string  })</pre> n/a yes idle_config List of time periods, defined as a cron expression, to keep a minimum amount of runners active instead of scaling down to 0. By defining this list you can ensure that in time periods that match the cron expression within 5 seconds a runner is kept idle. <pre>list(object({    cron             = string    timeZone         = string    idleCount        = number    evictionStrategy = optional(string, \"oldest_first\")  }))</pre> <code>[]</code> no instance_allocation_strategy The allocation strategy for spot instances. AWS recommends using <code>price-capacity-optimized</code> however the AWS default is <code>lowest-price</code>. <code>string</code> <code>\"lowest-price\"</code> no instance_max_spot_price Max price price for spot instances per hour. This variable will be passed to the create fleet as max spot price for the fleet. <code>string</code> <code>null</code> no instance_profile_path The path that will be added to the instance_profile, if not set the environment name will be used. <code>string</code> <code>null</code> no instance_target_capacity_type Default lifecycle used for runner instances, can be either <code>spot</code> or <code>on-demand</code>. <code>string</code> <code>\"spot\"</code> no instance_types List of instance types for the action runner. Defaults are based on runner_os (al2023 for linux and Windows Server Core for win). <code>list(string)</code> <pre>[  \"m5.large\",  \"c5.large\"]</pre> no job_queue_retention_in_seconds The number of seconds the job is held in the queue before it is purged. <code>number</code> <code>86400</code> no key_name Key pair name <code>string</code> <code>null</code> no kms_key_arn Optional CMK Key ARN to be used for Parameter Store. This key must be in the current account. <code>string</code> <code>null</code> no lambda_architecture AWS Lambda architecture. Lambda functions using Graviton processors ('arm64') tend to have better price/performance than 'x86_64' functions. <code>string</code> <code>\"arm64\"</code> no lambda_principals (Optional) add extra principals to the role created for execution of the lambda, e.g. for local testing. <pre>list(object({    type        = string    identifiers = list(string)  }))</pre> <code>[]</code> no lambda_runtime AWS Lambda runtime. <code>string</code> <code>\"nodejs18.x\"</code> no lambda_s3_bucket S3 bucket from which to specify lambda functions. This is an alternative to providing local files directly. <code>string</code> <code>null</code> no lambda_security_group_ids List of security group IDs associated with the Lambda function. <code>list(string)</code> <code>[]</code> no lambda_subnet_ids List of subnets in which the action runners will be launched, the subnets needs to be subnets in the <code>vpc_id</code>. <code>list(string)</code> <code>[]</code> no lambda_tracing_mode DEPRECATED: Replaced by <code>tracing_config</code>. <code>string</code> <code>null</code> no log_level Logging level for lambda logging. Valid values are  'silly', 'trace', 'debug', 'info', 'warn', 'error', 'fatal'. <code>string</code> <code>\"info\"</code> no logging_kms_key_id Specifies the kms key id to encrypt the logs with. <code>string</code> <code>null</code> no logging_retention_in_days Specifies the number of days you want to retain log events for the lambda log group. Possible values are: 0, 1, 3, 5, 7, 14, 30, 60, 90, 120, 150, 180, 365, 400, 545, 731, 1827, and 3653. <code>number</code> <code>180</code> no minimum_running_time_in_minutes The time an ec2 action runner should be running at minimum before terminated, if not busy. <code>number</code> <code>null</code> no pool_config The configuration for updating the pool. The <code>pool_size</code> to adjust to by the events triggered by the <code>schedule_expression</code>. For example you can configure a cron expression for weekdays to adjust the pool to 10 and another expression for the weekend to adjust the pool to 1. <pre>list(object({    schedule_expression = string    size                = number  }))</pre> <code>[]</code> no pool_lambda_reserved_concurrent_executions Amount of reserved concurrent executions for the scale-up lambda function. A value of 0 disables lambda from being triggered and -1 removes any concurrency limitations. <code>number</code> <code>1</code> no pool_lambda_timeout Time out for the pool lambda in seconds. <code>number</code> <code>60</code> no pool_runner_owner The pool will deploy runners to the GitHub org ID, set this value to the org to which you want the runners deployed. Repo level is not supported. <code>string</code> <code>null</code> no prefix The prefix used for naming resources <code>string</code> <code>\"github-actions\"</code> no queue_encryption Configure how data on queues managed by the modules in ecrypted at REST. Options are encryped via SSE, non encrypted and via KMSS. By default encryptes via SSE is enabled. See for more details the Terraform <code>aws_sqs_queue</code> resource https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/sqs_queue. <pre>object({    kms_data_key_reuse_period_seconds = number    kms_master_key_id                 = string    sqs_managed_sse_enabled           = bool  })</pre> <pre>{  \"kms_data_key_reuse_period_seconds\": null,  \"kms_master_key_id\": null,  \"sqs_managed_sse_enabled\": true}</pre> no redrive_build_queue Set options to attach (optional) a dead letter queue to the build queue, the queue between the webhook and the scale up lambda. You have the following options. 1. Disable by setting <code>enabled</code> to false. 2. Enable by setting <code>enabled</code> to <code>true</code>, <code>maxReceiveCount</code> to a number of max retries. <pre>object({    enabled         = bool    maxReceiveCount = number  })</pre> <pre>{  \"enabled\": false,  \"maxReceiveCount\": null}</pre> no repository_white_list List of github repository full names (owner/repo_name) that will be allowed to use the github app. Leave empty for no filtering. <code>list(string)</code> <code>[]</code> no role_path The path that will be added to role path for created roles, if not set the environment name will be used. <code>string</code> <code>null</code> no role_permissions_boundary Permissions boundary that will be added to the created roles. <code>string</code> <code>null</code> no runner_additional_security_group_ids (optional) List of additional security groups IDs to apply to the runner. <code>list(string)</code> <code>[]</code> no runner_architecture The platform architecture of the runner instance_type. <code>string</code> <code>\"x64\"</code> no runner_as_root Run the action runner under the root user. Variable <code>runner_run_as</code> will be ignored. <code>bool</code> <code>false</code> no runner_binaries_s3_logging_bucket Bucket for action runner distribution bucket access logging. <code>string</code> <code>null</code> no runner_binaries_s3_logging_bucket_prefix Bucket prefix for action runner distribution bucket access logging. <code>string</code> <code>null</code> no runner_binaries_s3_sse_configuration Map containing server-side encryption configuration for runner-binaries S3 bucket. <code>any</code> <pre>{  \"rule\": {    \"apply_server_side_encryption_by_default\": {      \"sse_algorithm\": \"AES256\"    }  }}</pre> no runner_binaries_s3_versioning Status of S3 versioning for runner-binaries S3 bucket. Once set to Enabled the change cannot be reverted via Terraform! <code>string</code> <code>\"Disabled\"</code> no runner_binaries_syncer_lambda_timeout Time out of the binaries sync lambda in seconds. <code>number</code> <code>300</code> no runner_binaries_syncer_lambda_zip File location of the binaries sync lambda zip file. <code>string</code> <code>null</code> no runner_boot_time_in_minutes The minimum time for an EC2 runner to boot and register as a runner. <code>number</code> <code>5</code> no runner_credit_specification The credit option for CPU usage of a T instance. Can be unset, \"standard\" or \"unlimited\". <code>string</code> <code>null</code> no runner_ec2_tags Map of tags that will be added to the launch template instance tag specifications. <code>map(string)</code> <code>{}</code> no runner_egress_rules List of egress rules for the GitHub runner instances. <pre>list(object({    cidr_blocks      = list(string)    ipv6_cidr_blocks = list(string)    prefix_list_ids  = list(string)    from_port        = number    protocol         = string    security_groups  = list(string)    self             = bool    to_port          = number    description      = string  }))</pre> <pre>[  {    \"cidr_blocks\": [      \"0.0.0.0/0\"    ],    \"description\": null,    \"from_port\": 0,    \"ipv6_cidr_blocks\": [      \"::/0\"    ],    \"prefix_list_ids\": null,    \"protocol\": \"-1\",    \"security_groups\": null,    \"self\": null,    \"to_port\": 0  }]</pre> no runner_extra_labels Extra (custom) labels for the runners (GitHub). Labels checks on the webhook can be enforced by setting <code>enable_workflow_job_labels_check</code>. GitHub read-only labels should not be provided. <code>list(string)</code> <code>[]</code> no runner_group_name Name of the runner group. <code>string</code> <code>\"Default\"</code> no runner_iam_role_managed_policy_arns Attach AWS or customer-managed IAM policies (by ARN) to the runner IAM role <code>list(string)</code> <code>[]</code> no runner_log_files (optional) Replaces the module default cloudwatch log config. See https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Agent-Configuration-File-Details.html for details. <pre>list(object({    log_group_name   = string    prefix_log_group = bool    file_path        = string    log_stream_name  = string  }))</pre> <code>null</code> no runner_metadata_options Metadata options for the ec2 runner instances. By default, the module uses metadata tags for bootstrapping the runner, only disable <code>instance_metadata_tags</code> when using custom scripts for starting the runner. <code>map(any)</code> <pre>{  \"http_endpoint\": \"enabled\",  \"http_put_response_hop_limit\": 1,  \"http_tokens\": \"required\",  \"instance_metadata_tags\": \"enabled\"}</pre> no runner_name_prefix The prefix used for the GitHub runner name. The prefix will be used in the default start script to prefix the instance name when register the runner in GitHub. The value is availabe via an EC2 tag 'ghr:runner_name_prefix'. <code>string</code> <code>\"\"</code> no runner_os The EC2 Operating System type to use for action runner instances (linux,windows). <code>string</code> <code>\"linux\"</code> no runner_run_as Run the GitHub actions agent as user. <code>string</code> <code>\"ec2-user\"</code> no runners_lambda_s3_key S3 key for runners lambda function. Required if using S3 bucket to specify lambdas. <code>string</code> <code>null</code> no runners_lambda_s3_object_version S3 object version for runners lambda function. Useful if S3 versioning is enabled on source bucket. <code>string</code> <code>null</code> no runners_lambda_zip File location of the lambda zip file for scaling runners. <code>string</code> <code>null</code> no runners_maximum_count The maximum number of runners that will be created. <code>number</code> <code>3</code> no runners_scale_down_lambda_timeout Time out for the scale down lambda in seconds. <code>number</code> <code>60</code> no runners_scale_up_lambda_timeout Time out for the scale up lambda in seconds. <code>number</code> <code>30</code> no runners_ssm_housekeeper Configuration for the SSM housekeeper lambda. This lambda deletes token / JIT config from SSM. <code>schedule_expression</code>: is used to configure the schedule for the lambda. <code>enabled</code>: enable or disable the lambda trigger via the EventBridge. <code>lambda_timeout</code>: timeout for the lambda in seconds. <code>config</code>: configuration for the lambda function. Token path will be read by default from the module. <pre>object({    schedule_expression = optional(string, \"rate(1 day)\")    enabled             = optional(bool, true)    lambda_timeout      = optional(number, 60)    config = object({      tokenPath      = optional(string)      minimumDaysOld = optional(number, 1)      dryRun         = optional(bool, false)    })  })</pre> <pre>{  \"config\": {}}</pre> no scale_down_schedule_expression Scheduler expression to check every x for scale down. <code>string</code> <code>\"cron(*/5 * * * ? *)\"</code> no scale_up_reserved_concurrent_executions Amount of reserved concurrent executions for the scale-up lambda function. A value of 0 disables lambda from being triggered and -1 removes any concurrency limitations. <code>number</code> <code>1</code> no ssm_paths The root path used in SSM to store configuration and secrets. <pre>object({    root       = optional(string, \"github-action-runners\")    app        = optional(string, \"app\")    runners    = optional(string, \"runners\")    use_prefix = optional(bool, true)  })</pre> <code>{}</code> no subnet_ids List of subnets in which the action runner instances will be launched. The subnets need to exist in the configured VPC (<code>vpc_id</code>), and must reside in different availability zones (see https://github.com/philips-labs/terraform-aws-github-runner/issues/2904) <code>list(string)</code> n/a yes syncer_lambda_s3_key S3 key for syncer lambda function. Required if using an S3 bucket to specify lambdas. <code>string</code> <code>null</code> no syncer_lambda_s3_object_version S3 object version for syncer lambda function. Useful if S3 versioning is enabled on source bucket. <code>string</code> <code>null</code> no tags Map of tags that will be added to created resources. By default resources will be tagged with name and environment. <code>map(string)</code> <code>{}</code> no tracing_config Configuration for lambda tracing. <pre>object({    mode                  = optional(string, null)    capture_http_requests = optional(bool, false)    capture_error         = optional(bool, false)  })</pre> <code>{}</code> no userdata_post_install Script to be ran after the GitHub Actions runner is installed on the EC2 instances <code>string</code> <code>\"\"</code> no userdata_pre_install Script to be ran before the GitHub Actions runner is installed on the EC2 instances <code>string</code> <code>\"\"</code> no userdata_template Alternative user-data template, replacing the default template. By providing your own user_data you have to take care of installing all required software, including the action runner. Variables userdata_pre/post_install are ignored. <code>string</code> <code>null</code> no vpc_id The VPC for security groups of the action runners. <code>string</code> n/a yes webhook_lambda_apigateway_access_log_settings Access log settings for webhook API gateway. <pre>object({    destination_arn = string    format          = string  })</pre> <code>null</code> no webhook_lambda_s3_key S3 key for webhook lambda function. Required if using S3 bucket to specify lambdas. <code>string</code> <code>null</code> no webhook_lambda_s3_object_version S3 object version for webhook lambda function. Useful if S3 versioning is enabled on source bucket. <code>string</code> <code>null</code> no webhook_lambda_timeout Time out of the webhook lambda in seconds. <code>number</code> <code>10</code> no webhook_lambda_zip File location of the webhook lambda zip file. <code>string</code> <code>null</code> no workflow_job_queue_configuration Configuration options for workflow job queue which is only applicable if the flag enable_workflow_job_events_queue is set to true. <pre>object({    delay_seconds              = number    visibility_timeout_seconds = number    message_retention_seconds  = number  })</pre> <pre>{  \"delay_seconds\": null,  \"message_retention_seconds\": null,  \"visibility_timeout_seconds\": null}</pre> no"},{"location":"modules/runners.html#outputs","title":"Outputs","text":"Name Description binaries_syncer n/a queues SQS queues. runners n/a ssm_parameters n/a webhook n/a"}]}